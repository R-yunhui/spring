# Mysql

## 基本概念

### 页

```
页是innodb读取磁盘的最小单位，整页整页的读取

页分为：数据页（BTreeNode），Undo页（undo Log page），系统页（Systempage）,事务数据页（Transaction SystemPage）每个数据页的大小为16kb，每个page使用一个32位int值来表示。
```

​																													**数据页结构**

| 名称               | 中文名             | 占用空间 | 描述                       |
| ------------------ | ------------------ | -------- | -------------------------- |
| File Header        | 文件头部           | 38字节   | 数据页的一些通用信息       |
| Page Header        | 页面头部           | 56字节   | 数据页的一些专有信息       |
| Infimum + supermum | 最小记录和最大记录 | 26字节   | 两个虚拟的行记录           |
| User Records       | 用户记录           | 不确定   | 实际存储的行记录内容       |
| Free Space         | 空闲空间           | 不确定   | 数据页中尚未使用的空间     |
| Pgae Directory     | 页面目录           | 不确定   | 数据页中某些记录的相对位置 |
| File Tailer        | 文件尾部           | 8字节    | 校验数据页是否完整         |

### B+树

​																															**B+树**

```
b+tree中，一个节点记录的都是key值和指针（排除了data值），这样就可以记录更多的key值，其key值是升序排序。

推算：innodb存储引擎一页的大小为16kb，一般主键类型为int占4个字节，或bigint占8个字节，指针类型页一般4个或8个字节，也就是说一个页中大概存储16*1024/16=1024个key，深度为3的B+tree索引（三页）可以维护1024*1024*1024=10亿条记录，实际情况每个节点不可能填满，B+tree的高度一般为2-4曾，mysql的innodb存储引擎将根节点常驻内存，也就是查找某一键值的行记录最多需要1-3次磁盘IO操作

B+tree和b-tree有如下不同点：
1.非叶子节点只存储键值信息（key信息）
2.所有叶子节点之间都有一个链指针，数据都存放再叶子节点中
```



### 锁的兼容情况

#### 表锁

|                | IS（读意向锁） | IX（写意向锁） | S（读锁） | X（写锁） | AI（自增锁） |
| -------------- | -------------- | -------------- | --------- | --------- | ------------ |
| IS（读意向锁） | 兼容           | 兼容           | 兼容      | 冲突      | 兼容         |
| IX（写意向锁） | 兼容           | 兼容           | 冲突      | 冲突      | 兼容         |
| S（读锁）      | 兼容           | 冲突           | 兼容      | 冲突      | 冲突         |
| X（写锁）      | 冲突           | 冲突           | 冲突      | 冲突      | 冲突         |
| AI（自增锁）   | 兼容           | 兼容           | 冲突      | 冲突      | 冲突         |

```
1.意向锁之间互不冲突；
2.S 锁只和 S/IS 锁兼容，和其他锁都冲突；
3.X 锁和其他所有锁都冲突；
4.AI 锁只和意向锁兼容；
```



#### 行锁

|                      | RECORD（记录锁） | GAP（意向锁） | NEXT-KEY（临键锁） | II GAP（插入意向锁） |
| -------------------- | ---------------- | ------------- | ------------------ | -------------------- |
| RECORD（记录锁）     | 冲突             | 兼容          | 冲突               | 兼容                 |
| GAP（意向锁）        | 兼容             | 兼容          | 兼容               | 兼容                 |
| NEXT-KEY（临键锁）   | 冲突             | 兼容          | 冲突               | 兼容                 |
| II GAP（插入意向锁） | 兼容             | 冲突          | 冲突               | 兼容                 |

```
第一行表示已有的锁，第一列表示要加的锁。插入意向锁较为特殊，所以我们先对插入意向锁做个总结，如下：
1.插入意向锁不影响其他事务加其他任何锁。也就是说，一个事务已经获取了插入意向锁，对其他事务是没有任何影响的；
2.插入意向锁与间隙锁和 Next-key 锁冲突。也就是说，一个事务想要获取插入意向锁，如果有其他事务已经加了间隙锁或 Next-key 锁，则会阻塞。

其他类型的锁的规则较为简单：
3.间隙锁不和其他锁（不包括插入意向锁）冲突；
4.记录锁和记录锁冲突，Next-key 锁和 Next-key 锁冲突，记录锁和 Next-key 锁冲突；
```



## 逻辑架构

​																									**MYSQL的逻辑架构图**

![image-20230109144940341](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109144940341.png)

### **连接器**

```mysql
连接器会在用户名和密码验证通过后，从权限表里面查询出你拥有的权限，之后这个连接里的权限判断都将依赖于此时读取到的权限。一个用户成功建立连接后，即使使用管理员账号对这个用户的权限进行了修改，也不会影响已经存在的连接的权限。修改后只有新建的连接会使用新的权限设置。

长连接：连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接：每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。（建立连接的过程通常是比较复杂的，建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。）

由于 MYSQL 在执行过程中临时使用的内存都是管理在连接对象里面的，这些资源会在连接断开的时候才释放。所以如果长连接累积下来可能导致内存占用太大，被系统强行杀掉（OOM）。

解决方案：
1.定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
2.如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。
```



### 查询缓存

```mysql
MYSQL 8.0之后没有查询缓存的功能
MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。

可以通过配置“按需使用”：
query_cache_type设置为DEMAND 即可

需要查询缓存的语句通过下述sql可见：
select SQL_CACHE * from T where ID=10;
```



### **分析器**

```mysql
词法分析：识别 sql 中的字符串分别是什么，代表什么。
语法分析：根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。
语义分析：判断语句是否正确，表是否存在，列是否存在等。
```



### **优化器**

```mysql
优化器是在表里面存在多个索引的时候，决定是用哪个索引；或者语句存在多表关联（join）的时候，决定各个表的连接顺序等。
```



### **执行器**

```sql
执行器需要先判断对这个表是否存在查询的权限（如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限）。如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。
```



## redolog + binlog

**update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。**

![image-20230109145043364](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109145043364.png)

​																											**update 语句执行流程**

```
WAL技术：Write-Ahead-Logging 先写日志，在写磁盘。

redolog 和 binlog 的不同点：
1.redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
2.redo log 是物理日志，记录的是“在某个数据页上做了什么修改”,是以数据页为单位的，线程共享的；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”，线程独享一部分内存。
3.redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

两段式提交的好处：
1.先写 redo log 后写 bin log。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。
但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。
然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。
2.先写 bin log 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。

redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。
sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。建议设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

redolog不是记录数据页“更新之后的状态”，而是记录这个页 “做了什么改动”。
binlog有两种模式，statement 格式的话是记sql语句， row格式会记录行的内容，记两条，更新前和更新后都有。

【注】：在正常情况下必须要执行完 redolog commit 才算结束，但是崩溃恢复的过程，可以接受“redolog prepare 并且 binglog 完整”的情况。

数据恢复：
binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果需要恢复半个月内的数据，那么备份系统中一定会保存最近半个月的所有 binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。
如果某天下午有误操作需要恢复数据，则可以：
1.首先，找到最近的一次全量备份，从这个备份恢复到临时库。
2.然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。
你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。

redolog有两种，一种记录普通数据页的改动，一种记录change buffer的改动
```



#### Mysql flush '脏页'

```
当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。
```

![1677742155157](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677742155157.png)

​																								**图 1 “孔乙己赊账”更新和 flush 过程**

```
可能引发数据库 flush 过程的场景：
1.innodb 的 redo log 写满了。这时候系统会停止所有更新的操作，将 checkpoint 向前推，redo log 留出空间可以继续写。如下图图2。
把 checkpoint 位置从 CP 推进到 CP’，就需要将两个点之间的日志（浅绿色部分），对应的所有脏页都 flush 到磁盘上。之后，图中从 write pos 到 CP’之间就是可以再写入的 redo log 的区域。

2.系统内存不足。当需要新的内存页但是内存不够用的时候，需要淘汰一部分数据页（最久不使用），空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。
从性能考虑。如果刷脏页一定会写磁盘，就保证了每个数据页有两种状态：
	2.1.内存里存在，内存里就肯定是正确的结果，直接返回； 
	2.2.内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。
这样的效率最高。

3.MySQL 认为系统“空闲”的时候。

4.MySQL 正常关闭的情况。这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。
```

![1677742223919](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677742223919.png)

​																										**图 2 redo log 状态图**

```
上述四种场景对于性能的影响：
1.场景三属于 MYSQL 空闲的时候的操作，系统不存在压力。
2.场景四属于数据库正常关闭，也不存在压力问题。
3.场景一“redo log 写满，需要 flush 脏页”，这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。
4.“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：
	4.1.还没有使用的；
	4.2.使用了并且是干净页；
	4.3.使用了并且是脏页。
InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。
而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。

所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：
1.一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
2.日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。
所以，InnoDB 需要有控制脏页比例的机制，来尽量避免上面的这两种情况。
```



#### InnoDB 刷脏页的控制策略

```
首先需要知道 innodb 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。
innodb_io_capacity 这个参数，它会告诉 InnoDB 你的磁盘能力。这个值我建议你设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句可以用来测试磁盘随机读写的命令：
fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest 

设计策略控制刷脏页的速度，会参考哪些因素？
1.内存中的脏页太多。（脏页比例）
2.rodo log写满。（redo log 写磁盘的速度）

参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%

InnoDB 每次写入的日志都有一个序号，当前写入的序号跟 checkpoint 对应的序号之间的差值，我们假设为 N。InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字，这个计算公式可以记为 F2(N)。F2(N) 算法比较复杂，你只要知道 N 越大，算出来的值越大就好了。

然后，根据上述算得的 F1(M) 和 F2(N) 两个值，取其中较大的值记为 R，之后引擎就可以按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度。
```

![1677742636720](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677742636720.png)

```
一个有趣的策略。

一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。

在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。

找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。

而如果使用的是 SSD 这类 IOPS 比较高的设备的话，我就建议你把 innodb_flush_neighbors 的值设置成 0。因为这时候 IOPS 往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。

在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。
```



### 思考题

```
一个内存配置为 128GB、innodb_io_capacity 设置为 20000 的大规格实例，正常会建议你将 redo log 设置成 4 个 1GB 的文件。

但如果你在配置的时候不慎将 redo log 设置成了 1 个 100M 的文件，会发生什么情况呢？又为什么会出现这样的情况呢？
```

![image-20220110162226720](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220110162226720.png)

```
每次事务提交都要写 redo log，如果设置太小，很快就会被写满，也就是上面这个图的状态，这个“环”将很快被写满，write pos 一直追着 CP。这时候系统不得不停止所有更新，去推进 checkpoint。（因为 checkpoint 一直要往前推，这个操作就会触发 merge 操作，然后又进一步地触发 flush 操作,导致 change buffer 的优化也失效了）
这时，你看到的现象就是磁盘压力很小，但是数据库出现间歇性的性能下跌。
```



### 总结

```
WAL机制后续需要的刷脏页操作和执行时机。利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。

但是，由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。

【注】：由于淘汰的时候，刷脏页过程不用动redo log文件的。这个有个额外的保证，是redo log在“重放”的时候，如果一个数据页已经是刷过的，会识别出来并跳过。

innodb 通过数据页头部的 LSN，8个字节，每次修改都会变大。（File Header 上：页面被最后修改时对应的日志序列位置）
对比这个数据页的LSN 跟 checkpoint 的 LSN，比checkpoint小的一定是干净页。
```



## 事务

### 隔离性和隔离级别

```
事务的特性：原子性，一致性，隔离性，持久性。

多个事务同时执行可能出现：脏读，不可重复读，幻读的问题。

事务的隔离级别：
1.读未提交(re uncommitted)：一个事务还没提交时，它做的变更就能被别的事务看到。
2.读提交(read committed)：一个事务提交之后，它做的变更才会被其他事务看到。
3.可重复读(repeatable read)：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。(核心是一致性读)
4.串行化(serializable)：顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：
1.在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
2.在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。
```

```
原子性：依赖 undo log
隔离性：依赖 MVCC 机制
持久性：依赖 redo log
一致性：依赖其它三个特性一起实现
```



### 事务隔离的实现

```
事务隔离在实现上面，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”的隔离级别下，这个视图是在事务启动的时候创建的，整个事务存在期间都在使用这个视图；在“读提交”的隔离级别下，这个视图是在 SQL 语句开始执行的时候创建；“读未提交”隔离级别下没有视图的概念，直接返回的是记录里面的最新值；而“串行化”的隔离级别下直接使用加锁的方式避免并行访问。
```

```
在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。
```

![image-20230109151016412](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109151016412.png)

```
当前的值是4，但是在查询此条记录的时候，不同时刻启动的事务会有不同的 read-view。如上图：在视图A,B,C中对应的值分别是1，2，4，同一条记录在系统中存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。

回滚日志不能一直保留，在不需要的时候才删除。系统会判断，当没有事务再需要用到这些回滚日志时（就是当系统里没有比这个回滚日志更早的 read-view 的时候），回滚日志会被删除。
```



### 一致性视图（MVCC）

```
【注】：
1.通过 begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 innodb 表的语句，事务才启动。一致性视图是在执行第一个快照读语句的时候创建的。
2.通过 start transaction with consistent snapshot 命令，可以立刻启动一个事务。一致性视图实在执行命令的时候会创建的。
【注】：第二个语法不适用于 读提交 的事务隔离级别，在 读提交 的事务隔离级别下，等效于普通的 begin/start transaction。
```

```
innoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 innoDB 的事务系统申请的，是按申请顺序严格递增的。

而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。

也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。

如下图所示，表示一个记录被多个事务连续更新后的状态：
```

![image-20230109151342994](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109151342994.png)

​																					**一个记录被多个事务连续更新后的状态**

```
上图中的虚线箭头就代表 undo log。而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。
```

![image-20230109151358666](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109151358666.png)

​																											**数据版本可见性规则**

```
如上图所示：
对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：
1.如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2.如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
3.如果落在黄色部分，那就包括两种情况
	3.1.若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；
	3.2.若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。
```



```
如上图所示，假设：
1.事务 A 开始前，系统里面只有一个活跃事务 ID 是 99；
2.事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务；
3.三个事务开始前，(1,1）这一行数据的 row trx_id 是 90。
```

![image-20230109151433715](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109151433715.png)

​																										**事务A查询数据逻辑图**

```
1.第一个有效的事务C，将数据从（1,1）改为了（1,2）。此时这个数据的最新版本为 102,而 90 这个版本已经成为了历史版本。
2.第二个有效的事务B，将数据从（1,2）改为了（1,3）。此时这个数据的最新版本为 101，而 102 这个版本也成为了了历史版本。
3.当事务A在查询的时候，事务B并没有提交，但是它生成的（1,3）这个版本已经变成了当前版本，但是这个版本对于事务A来说是不可见的。否则会出现脏读。
对于事务A来说它的视图数组：[99,100]。读数据是从当前版本开始的，也就是（1,3）的版本开始即102,。
	3.1.找到 (1,3) 的时候，判断出 row trx_id=101，比高水位大，处于红色区域，不可见；
	3.2.接着，找到上一个历史版本，一看 row trx_id=102，比高水位大，处于红色区域，不可见；
	3.3.再往前找，终于找到了（1,1)，它的 row trx_id=90，比低水位小，处于绿色区域，可见。
虽然这一行数据被修改过，但是事务A不论在任何时候进行查询，看到的这行数据的结果都是一致的，所以被称为一致性读。
```

![image-20230109151614198](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109151614198.png)

​																													**事务B查询数据逻辑图**

```
对于事务B来说，在更新数据之前需要先查询一次数据，这个返回的 k 的结果确实是1。但是当它需要去更新的时候，就不能再历史版本上面进行更新，否则事务C的更新就丢失了。因此，事务B的更新操作是在（1,2）的基础上进行的操作。
规则：更新数据都是先读后写的，而这个读，读取的是当前的值，称为：“当前读”。
```



```
规则总结：
1.版本未提交，不可见；
2.版本已提交，但是是在视图创建后提交的，不可见；
3.版本已提交，而且是在视图创建前提交的，可见。

对于事务A：
1.（1,3）的版本未提交，所以不可见。
2.（1,2）的版本已经提交，但是是在视图数组创建之后提交的，也不可见。
3.（1，1）是在视图数组创建之前提交的，可见。

如果事务A的 select 语句进行加锁：
select * from t where id = 1 for update;（加了写锁 X锁 排他锁）
select * from t where id = 1 lock in share mode;（加了读锁 S锁 共享锁）
也属于当前读。
```

![image-20230109151650893](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109151650893.png)

```
事务隔离级别：读提交
事务A：读提交的视图数组是在第一个 SQL 语句执行的时候创建的，所以（1,3）的版本是在事务A的视图数组创建之后提交的，不可见。（1,2）的版本是在事务A的视图数组创建之后创建的，可见。  k = 1
事务B：（1,2）未提交的时候，这一行被锁住（行锁），事务B对这一行的更新操作需要等到事务C提交之后，释放了锁才可以访问。说明事务B的更新操作需要做“当前读”，对于事务B来说（1,2）为当前版本，则更新之后读取到的为（1,3）。 k = 3
事务C：k = 2

事务隔离级别：可重复读
事务A：事务启动，则创建了视图数组。 k = 1
事务B：同上：k = 3
事务C：k = 2
```



#### 思考题

```sql
-- 用下面的表结构和初始化语句作为试验环境，事务隔离级别是可重复读。现在，我要把所有“字段 c 和 id 值相等的行”的 c 值清零，但是却发现了一个“诡异”的、改不掉的情况。请你构造出这种情况，并说明其原理。

CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
insert into t(id, c) values(1,1),(2,2),(3,3),(4,4);
```

![image-20230109151713725](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109151713725.png)

```
答：如上图所示，即可复现。另起一个事务，在 update 语句执行之前，先把所有 c 值进行修改（update t set c = c + 1;）。
```



### 总结 

```
InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。
1.对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
2.对于读提交，查询只承认在语句启动前就已经提交完成的数据；
而当前读，总是读取已经提交完成的最新版本。

数据版本的可见性规则总结：
1.版本未提交，不可见；
2.版本已提交，但是是在视图创建后提交的，不可见；
3.版本已提交，而且是在视图创建前提交的，可见。

【注】：innodb 需要保证一个规则：事务启动之前所有还没有提交的事务，它都不可见。
但是只保存一个已经提交事务的最大值是不够的的，存在那些比最大值小的事务，之后也可能会更新。所以事务启动的时候还需要保存“现在正在执行的所有事务ID列表”，如果一个 row trx_id 在列表中，也要不可见。
```



## 索引 

### InnoDB 的索引模型

![image-20230109151805946](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109151805946.png)

​																												**InnoDB 的索引组织结构**

```
索引是在存储引擎层实现的。 Innodb 使用的 B+ 树索引模型。每一个索引在 Innodb 里面对应一颗 B+树。

根据叶子节点的内容，索引类型分为主键索引和非主键索引。
1.主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。
2.非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。

基于非主键索引的查询需要多扫描一棵索引树（需要先查询到 k 索引树获取主键的值，在到 ID 索引树搜索一次，此过程称之为回表）。因此，在应用中应尽量使用主键查询。
```



### 索引维护

```
B+树 需要维护索引的有序性，在插入新值的时候需要做必须的维护。以上图为例：插入新的行 ID = 700，则追加在 R5后面即可，如果查询的 ID 值为400，则需要逻辑上挪动后面的数据，空出位置。
而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。（分裂过程中只会分裂它要写入的那个页面，每个页面之间是用指针链接的，改指针就可以，不需要“后面的全部挪动“）。
当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。

主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。
但是在典型的 k - v 场景下（1.只有一个索引 2.该索引是唯一索引），由于没有其它索引，所以不需要考虑其他索引的叶子结点大小的问题，则“尽量使用主键查询”的原则，直接将这个索引设置为主键，则可以避免每次查询需要搜索两棵树（回表的过程）。
```



### 覆盖索引

```sql
-- 建表
create table T (
ID int primary key,
k int NOT NULL DEFAULT 0, 
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;
 
-- 初始化
insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');

-- 执行查询 sql
select * from T where k between 3 and 5
```

```
1.在 k 索引树上找到 k = 3的记录，得到 ID = 300。
2.再到主键索引树上找到 ID = 300对应的记录（回表：回到主键索引树搜索的过程，我们称为回表）。
3.在 k 索引树上找到 k = 5的记录，得到 ID = 600。
4.再到主键索引树上找到 ID = 600对应的记录（回表）。
5.在 k 索引树上找到 k = 6的记录，不满足条件，循环结束。

如果通过 sql：select ID from T where k between 3 and 5; 
则不存在回表的过程，因为 k 索引树上已经存储了 ID 的值，满足查询的结果。索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。
由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段(某索引已经覆盖了查询需求，称为覆盖索引)。

创建联合索引，也可以在高频请求上面用到覆盖索引，不需要再进行回表查询整行的记录，较少语句的执行时间。
```



### 最左前缀原则

```sql
CREATE TABLE `tuser` (
	`id` INT ( 11 ) NOT NULL,
	`id_card` VARCHAR ( 32 ) DEFAULT NULL,
	`name` VARCHAR ( 32 ) DEFAULT NULL,
	`age` INT ( 11 ) DEFAULT NULL,
	`ismale` TINYINT ( 1 ) DEFAULT NULL,
	PRIMARY KEY ( `id` ),
	KEY `id_card` ( `id_card` ),
KEY `name_age` ( `name`, `age` ) 
) ENGINE = INNODB;
```

![image-20230109152130544](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109152130544.png)

​																												**（name，age）索引示意图**

```
当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得到所有需要的结果。

如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是"where name like ‘张 %’"。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。

可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。

在建立联合索引的时候，如何安排索引内的字段顺序？
1.如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的（比如：存在（name，age）这个联合索引，则一般就不需要再单独为 name 建立一个索引）。
2.如果业务存在必须同时维护（a,b）和（b）这俩索引，则考虑空间的原则（name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引）。
```



### 索引下推

```sql
select * from tuser where name like '张 %' and age = 10 and ismale = 1;
```

![image-20230109152222440](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109152222440.png)

​																														**图1.无索引下推过程**

![image-20230109152231891](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109152231891.png)

​																										**图2.索引下推过程**

```
MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

图 1 中，在 (name,age) 索引里面去掉了 age 的值，这个过程 InnoDB 并不会去看 age 的值，只是按顺序把“name 第一个字是’张’”的记录一条条取出来回表。因此，需要回表 4 次。

图 2 跟图 1 的区别是，InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。在我们的这个例子中，只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。
```



### 唯一索引和普通索引的选择

**见上图 InnoDB 的索引组织结构**

#### 查询过程

```
假设执行的sql：select id from t where k = 5;
先是在索引树上进行查找，通过 B+树 从树根开始，按层搜索叶子节点。定位到对应的数据页，之后在数据页内部使用二分来进行定位对应的记录。
1.对于普通索引：查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k = 5 条件的记录。
2.对于唯一索引：由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。
二者的性能差距基本不存在。
唯一复杂的一点是：如果 k = 5 刚好是这个数据页的最后一个记录，对于普通索引来说需要读取下一个数据页，而唯一索引不会。
```



#### change buffer

```
当需要更新一个数据页时，如果数据页在内存中则直接更新，而如果这个数据页没有在内存中，在不影响数据一致性的前提下，Innodb 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读取这个数据页。在下次查询需要访问这个数据页的时候，将数据页读入内存中，然后执行 change buffer 中的对这个数据页的操作。通过这种方式保证这个数据逻辑的正确性。

change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。
将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

对于唯一索引来说，每次进行更新操作需要先判断这个操作是否违反唯一性约数，必须要将数据页读入内存中才能判断。因此唯一索引不能使用 change buffer。实际上也只有普通索引可以使用 change buffer。
change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。

merge的过程：
1.从磁盘读入数据页到内存（老版本的数据页）；
2.从 change buffer 里找出这个数据页的 change buffer 记录 (可能有多个），依次应用，得到新版数据页；
3.写 redo log。这个 redo log 包含了数据页的变更和 change buffer 的变更。
到这里 merge 过程就结束了。这时候，数据页和内存中 change buffer 对应的磁盘位置都还没有修改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。
```



#### 更新过程

```
如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程：
1.这个记录更新的目标页在内存中：
	1.1.对应唯一索引来说，找到 3,5之间的位置，判断不存在冲突，插入这个值，结束。
	1.2.对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。
	
2.这个记录更新的目标页不在内存中：
	2.1.对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
	2.2.对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。
将数据从磁盘读入内存涉及到随机IO的访问，是数据库里面成本最高的操作之一。 change buffer 因为减少了随机磁盘访问，所以显著提高了性能。	
```



#### change buffer 的使用场景

```
1.写多读少的业务：页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。
2.写少读多的业务：写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。

如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。
实际使用中，你会发现，普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。
	特别地，在使用机械硬盘时，change buffer 这个机制的收效是非常显著的。所以，当你有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，那你应该特别关注这些表里的索引，尽量使用普通索引，然后把 change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。
```



#### change buffer 和 redo log

```sql
insert into t(id,k) values(id1,k1),(id2,k2);
```

![1677744833172](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677744833172.png)

​																										**带change buffer 的更新过程**

```
假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。如上图所示：
这条更新语句做了如下的操作（按照图中的数字顺序）：
1.Page1 在内存中，直接更新内存
2.Page2 不在内存中，就在内存中的 change buffer 中记录“向Page2 插入一行的信息”。
3.将上述的操作计入 redo log中。图中的3和4）（顺序写）

执行上述之后事务结束，写了两次内存操作，写了一次磁盘操作（两次操作合在一起写了一次磁盘）。
```



```sql
select * from t where k in (k1, k2);
```

![1677744848545](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677744848545.png)

​																											 **带change buffer 的读过程**

```
1.读 Page 1 的时候，直接从内存返回。WAL 之后如果读数据，是不是一定要读磁盘，是不是一定要从 redo log 里面把数据更新以后才可以返回？
是不用的。可以看一下图 3 的这个状态，虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的。
2.要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果（直到需要读 Page 2 的时候，这个数据页才会被读入内存）。

结论：
简单地对比这两个机制（change buffer 和 redo log）在提升更新性能上的收益的话，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。
```



### 优化器的逻辑

```sql
CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `a` int(11) DEFAULT NULL,
  `b` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `a` (`a`),
  KEY `b` (`b`)
) ENGINE=InnoDB;

-- 存储过程存入 100000 条数据
delimiter;
create procedure idata()
begin
  declare i int;
  set i=1;
  while(i<=100000)do
    insert into t values(i, i, i);
    set i=i+1;
  end while;
end;
delimiter ;
call idata();
```

```sql
-- 第一种情况
select * from t where a between 10000 and 20000;

explain select * from t where a between 10000 and 20000;
```

![image-20220110143635751](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220110143635751.png)

​																											**第一种逻辑的分析**

```
优化器使用的是索引 ‘a’。
```



![image-20220110144441462](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220110144441462.png)

​																															**第二种情况**

```sql
set long_query_time = 0;
select * from t where a between 10000 and 20000; /*Q1*/
select * from t force index(a) where a between 10000 and 20000;/*Q2*/
```

```
第一句，是将慢查询日志的阈值设置为 0，表示这个线程接下来的语句都会被记录入慢查询日志中；
第二句，Q1 是 session B 原来的查询；
第三句，Q2 是加了 force index(a) 来和 session B 原来的查询语句执行情况对比。
```



![image-20220110144556569](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220110144556569.png)

​																									**第二种情况的慢sql日志**

```
优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。

MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。

这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。

我们可以使用 show index 方法，看到一个索引的基数。虽然这个表的每一行的三个字段值都是一样的，但是在统计信息中，这三个索引的基数值并不同，而且其实都不准确。

Mysql通过采样统计的方式获取索引的基数：
采样统计的时候，InnoDB 默认会选择 N 个数据页（哪个索引的基数就是在哪个索引树上拿），统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。
而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。
在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择：
1.设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。
2.设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16。

可以通过： analyze table t; 进行重新统计索引信息。
```



```sql
select * from t where (a between 1 and 1000)  and (b between 50000 and 100000) order by b limit 1;
```

```
分析：1.如果使用索引 a 进行查询，则是扫描索引 a 的前1000个值，然后取出对应的主键id，再去主键索引去查出每一行，然后根据字段 b进行过滤。需要扫描1000行数据。
2.如果使用索引 b 进行查询，那么就是扫描索引 b 的最后 50001 个值，与上面的执行过程相同，也是需要回到主键索引上取值再判断，所以需要扫描 50001 行。
```

![image-20220110145150665](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220110145150665.png)

​																										**上述sql分析结果**

```
优化器使用的是索引 'b',需要扫描的行数 51479,
1.扫描行数的估计值依然是不准的。
2.mysql 选择了错误的所有。
```



#### 索引选择异常和处理

```
1.采用force index 强行选择一个索引。
MySQL 会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行。如果 force index 指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执行代价。

2.考虑修改语句，引导 MySQL 使用我们期望的索引。
如上面的那个sql可以调整为：(由于存在 limit 1，则说明逻辑上一致的，可以修改为下面的sql)
select * from t where (a between 1 and 1000)  and (b between 50000 and 100000) order by b,a limit 1;
之前优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序（b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。

现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 行的索引 a。

3.在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。
```



### 如何给字符串字段加索引

#### 前缀索引和普通索引

```sql
create table SUser(
ID bigint unsigned primary key,
email varchar(64), 
name varchar(33)
)engine=innodb; 

select f1, f2 from SUser where email='xxx';
```

```
1.如果 email 字段上没有加索引，则会走全表扫描。

2.如下方式增加索引：
	2.1.alter table SUser add index index1(email);
	2.2.alter table SUser add index index2(email(6));
第一个语句创建的 index1 索引，包含了每个记录的整个 email 字符串；而第二个索引只有前 6 个字节。
前缀索引相对而言占用的空间会更少，这是前缀索引的优势，但是同时带来的可能会增加额外的记录扫描行数。
```

![1677745039736](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677745039736.png)

​																										**图 1 email 索引结构**



![1677745049480](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677745049480.png)

​																										**图 2 email(6) 索引结构**

```
select id,name,email from SUser where email='zhangssxyz@xxx.com';

如何使用 index1：
1.从 index1 索引树找到满足索引值是 'zhangssxyz@xxx.com' d的记录，取出 ID2的值。
2.到主键上查到主键值是 ID2 的行，判断 email 的值是正确的，将这行记录加入结果集；
3.取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足 email='zhangssxyz@xxx.com’的条件了，循环结束。

如何使用 index2：
1.从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1;
2.到主键上查到主键值是 ID1 的行，判断出 email 的值不是’zhangssxyz@xxx.com’，这行记录丢弃;
3.取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到 ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集;
4.重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。

【注】：使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。
```

```
我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。
1.算出这个列上有多少个不同的值
select count(distinct email) as L from SUser;

2.依次选取不同长度的前缀来看这个值，比如我们要看一下 4~7 个字节的前缀索引
select 
  count(distinct left(email,4)）as L4,
  count(distinct left(email,5)）as L5,
  count(distinct left(email,6)）as L6,
  count(distinct left(email,7)）as L7,
from SUser;
```

```
select id,email from SUser where email='zhangssxyz@xxx.com';

如果使用 index1（即 email 整个字符串的索引结构）的话，可以利用覆盖索引，从 index1 查到结果后直接就返回了，不需要回到 ID 索引再去查一次。而如果使用 index2（即 email(6) 索引结构）的话，就不得不回到 ID 索引再去判断 email 字段的值。

即使你将 index2 的定义修改为 email(18) 的前缀索引，这时候虽然 index2 已经包含了所有的信息，但 InnoDB 还是要回到 id 索引再查一下，因为系统并不确定前缀索引的定义是否截断了完整信息。

也就是说，使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。
```



#### 其它方式

```
第一种方式是使用倒序存储。
例如身份证号码，一共 18 位，其中前 6 位是地址码，所以同一个县的人的身份证号前 6 位一般会是相同的。由于身份证号的最后 6 位没有地址码这样的重复逻辑，所以最后这 6 位很可能就提供了足够的区分度。也需要通过 count(distinct()) 的方式做验证。

第二种方式使用 hash 字段。
你可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。
因为可能存在 hash 冲突，所以查询语句 where 部分要判断 id_card 的值是否精确相同。

异同点：
首先，它们的相同点是，都不支持范围查询。倒序存储的字段上创建的索引是按照倒序字符串的方式排序的，已经没有办法利用索引方式查出身份证号码在 [ID_X, ID_Y] 的所有市民了。同样地，hash 字段的方式也只能支持等值查询。

它们的区别，主要体现在以下三个方面：

1.从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。

2.在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。

3.从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。
```



### 思考题

#### 思考题一

```
create table T(
id int primary key, 
k int not null, 
name varchar(16),
index (k))engine=InnoDB;

重建索引 k
alter table T drop index k;
alter table T add index(k);

重建主键索引
alter table T drop primary key;
alter table T add primary key(id);

对于上面这两个重建索引的作法，说出你的理解。如果有不合适的，为什么，更好的方法是什么？
答：对于重建索引 K 的做法是合理的，可以达到省空间的目的。但是重建主键的过程不合理，不论是重建主键还是创建主键，都会将这个表继续重建。所有如果连这执行这俩预警则第一个重建索引 k 的操作就白费了。可以使用：alter table T engine = InnoDB; 进行代替。 
```

```
CREATE TABLE `geek` (
  `a` int(11) NOT NULL,
  `b` int(11) NOT NULL,
  `c` int(11) NOT NULL,
  `d` int(11) NOT NULL,
  PRIMARY KEY (`a`,`b`),
  KEY `c` (`c`),
  KEY `ca` (`c`,`a`),
  KEY `cb` (`c`,`b`)
) ENGINE=InnoDB;

公司的同事告诉他说，由于历史原因，这个表需要 a、b 做联合主键，这个小吕理解了。

但是，学过本章内容的小吕又纳闷了，既然主键包含了 a、b 这两个字段，那意味着单独在字段 c 上创建一个索引，就已经包含了三个字段了呀，为什么要创建“ca”“cb”这两个索引？

同事告诉他，是因为他们的业务里面有这样的两种语句：

select * from geek where c = N order by a limit 1;
select * from geek where c = N order by b limit 1;

这位同事的解释对吗，为了这两个查询模式，这两个索引是否都是必须的？为什么呢？
答：InnoDB会把主键字段放到索引定义字段后面。主键（a，b）的聚簇索引组织顺序相当于order by a,b，也就是先按照a排序，在按照b排序，c无序的；对于索引（c，a）的组织是先按照 c 排序，然后 a 排序（和单独按照索引 c 进行排序是一样的），同时记录主键；对于索引（c，b）的组织是先按照 c 排序，然后 c 排序，同时记录主键。
所以 （c，a）索引是可以干掉的，（c，b）需要保留。定义为 c 的索引，实际是（c，a，b），定义为（c，a）的索引实际上是（c，a，b）。
```



#### 思考题二

```
通过上图（带change buffer 的更新过程）可以看到，change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？
答：不会丢失。change buffer有一部分在内存有一部分在ibdata。磁盘中持久化的change buffer 数据已经 merge了，不需要回复。没有持久化的数据可以通过redo log，虽然是只更新内存，但是在事务提交的时候，我们把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。
```



#### 思考题三

```
上面例子：通过 session A 的配合，让 session B 删除数据后又重新插入了一遍数据，然后就发现 explain 结果中，rows 字段从 10001 变成 37000 多。而如果没有 session A 的配合，只单独执行 delete from t 、call idata()、explain 这三句话，会看到 rows 字段其实还是 10000 左右。你可以自己验证一下这个结果。

答：delete 语句删掉了所有的数据，然后再通过 call idata() 插入了 10 万行数据，看上去是覆盖了原来的 10 万行。

但是，session A 开启了事务并没有提交，所以之前插入的 10 万行数据是不能删除的。这样，之前的数据每一行数据都有两个版本，旧版本是 delete 之前的数据，新版本是标记为 deleted 的数据。

这样，索引 a 上的数据其实就有两份。

主键上的数据也不能删，那没有使用 force index 的语句，使用 explain 命令看到的扫描行数为什么还是 100000 左右？

是的，不过这个是主键，主键是直接按照表的行数来估计的。而表的行数，优化器直接用的是 show table status 的值。
```



#### 思考题四

```
如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是”学号 @gmail.com", 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。

系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢？

答：由于这个学号的规则，无论是正向还是反向的前缀索引，重复度都比较高。因为维护的只是一个学校的，因此前面 6 位（其中，前三位是所在城市编号、第四到第六位是学校编号）其实是固定的，邮箱后缀都是 @gamil.com，因此可以只存入学年份加顺序编号，它们的长度是 9 位。

而其实在此基础上，可以用数字类型来存这 9 位数字。比如 201100001，这样只需要占 4 个字节。其实这个就是一种 hash，只是它用了最简单的转换规则：字符串转数字的规则，而刚好我们设定的这个背景，可以保证这个转换后结果的唯一性。

【注】：考虑到使用的场景，这个表大概率疏通费小表，为了业务简单，可以直接使用此字符串当索引。
```



### 总结

```
1.索引的作用：提高数据查询效率。
2.常见的索引模型：哈希表，有序数组，搜索树。
	2.1.把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。针对多个 key 值经过哈希函数的计算后出现同一个值的情况，引入一个链表进行处理。哈希表这种结构适用于只有等值查询的场景。
	2.2.有序数组在等值查询和范围查询场景中的性能就都非常优秀，但是只适用于静态存储引擎。仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。
	2.3.二叉搜索树的查询复杂度 O(log(N))，需要保持这棵树是平衡二叉树。更新的时间复杂度也是 O(log(N))。
3.主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。在特殊场景下，遵从最节省空间的原则。
4.针对高频查询：可以建立联合索引来使用覆盖索引，不需要进行回表的操作。
5.针对非高频查询：再已有的联合索引的基础上，使用最左前缀原则进行匹配来快速查询。
6.对于mysql 5.6及以上的引入了索引下推，减少回表的次数。
7.数据量很大的时候，二级索引比主键索引更快的情况只适用于在使用覆盖索引的情况下，非覆盖所有依然需要回表的操作。
8.覆盖索引：如果查询条件使用的是普通索引（或是联合索引的最左原则字段），查询结果是联合索引的字段或是主键，不用回表操作，直接返回结果，减少IO磁盘读写。
9.最左前缀原则：是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符，只要满足最左前缀，就可以利用索引来加速检索。
10.索引下推：可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。
11.对于唯一索引来说，每次进行更新操作需要先判断这个操作是否违反唯一性约束，必须要将数据页读入内存中才能判断。因此唯一索引不能使用 change buffer。实际上也只有普通索引可以使用 change buffer。
12.简单地对比这两个机制（change buffer 和 redo log）在提升更新性能上的收益的话，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要减少的则是随机读磁盘的 IO 消耗。
13.锁是一个单独的数据结构，如果数据页上有锁，change buffer 在判断“是否能用”的时候，就会认为否。
14.change buffer跟普通数据页一样也是存在磁盘里，区别在于change buffer是在系统表空间 ibdata1 里。
15.insert 的时候，写主键是不能用change buffer了，但是同时也会要写其它索引，而其它索引中的“非唯一索引”是可以用的这个change buffer的；
16.直接创建完整索引，这样可能比较占用空间；
17.创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；
18.倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；
19.创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。
```



## 锁

```
根据加锁的范围，MYSQL 里面的锁大致可以分为全局锁，表级锁，行锁三类
```

### 全局锁

```
全局锁是对整个数据库实例加锁：flush tables with read lock；让整个库处于只读状态，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

FTWRL 前存在读写的话，会进行等待，读写执行完毕之后才会执行。

典型的使用场景：全库的逻辑备份。
存在的风险：
1.主库备份，备份期间都不能执行更新，业务基本停滞。
2.从库备份，备份期间从库不能执行从主库同步过来的 binlog，导致主从延迟。

全库只读，为什么不使用 set global readonly=true 的方式呢？确实 readonly 方式也可以让全库进入只读状态，但我还是会建议你用 FTWRL 方式，主要有两个原因：
一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。
二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。
```



### 表级锁

```
MSYQL 中的表级锁存在两种：
1.表锁
2.元数据锁（meta data lock， MDL） -> 为了防止 DDL 和 DML 并发的冲突。

表锁的语法：lock tables ... read/write;(通过 unlock tables; 释放锁)

元数据锁（MDL）不需要显示的使用，在访问一个表的时候被自动加上。MDL锁的作用是，保证读写的正确性。
MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。
1.读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
2.读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。
```



![image-20230109162210994](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230109162210994.png)

​																															**MDL锁**

```
 如上图：session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。
 之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。
 之后所有对表进行增删改查的操作都需要先申请 MDL 读锁，就都会被锁住，等价于表 t 完全不可读写。
 如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。
 
 【注】：事务中的 MDL 锁，是在语句执行开始的时候进行申请，但是语句结束并不会立马释放，而会等到整个事务提交之后才会释放。
```



### 行锁

```
两阶段锁协议：在 innodb 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是需要等到事务结束的时候才会释放锁。
【注】：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

innodb 的行级锁是通过给索引上的索引项加锁来实现的，如果 update 的列没有建索引，那么会使用表锁，内部是全表根据主键索引逐行进行扫描，逐行加锁，在事务提交之后释放锁（两段式提交）。
```



#### 死锁和死锁检测

```
当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。
```

![image-20230110093536242](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230110093536242.png)

​																													**Mysql的死锁**

```
如上图所示：事务A在等待事务B释放 id = 2 这一行的行锁，事务B在等待事务A释放 id = 1 这一行的行锁。事务A和事务B互相等待对方的资源释放，就是进入了死锁的状态。两种解决策略：
1.直接进入等待，直到超时。超时时间可以通过参数 innodb_lock_wait_timeout（默认50s） 进行设置。
2.发起死锁检测，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。参数 innodb_deadlock_delect 设置为 on，表示开启死锁检测。
死锁检测：每当一个事务被锁的时候，就需要看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是是否出现了死锁。这是一个时间复杂度达到 O(n) 的操作。当存在 1000 个线程需要同时更新同一行的时候，那么死锁检测操作就是 百万 级别的，需要耗费大量的 CPU 资源。
	2.1.可以引入中间件进行并发的控制，或者直接在 mysql 端进行并发的控制，对于相同行的更新操作，在进入引擎之前进行排队，那么 innodb 内部就不会存在大量的死锁检测工作了。
	2.2.死锁检测并不是每条事务执行前都会进行检测而是需要加锁访问的行上有锁才需要检测。
【注】:
1.一致性读不会进行加锁，就不需要进行死锁检测。
2.并不是每次死锁检测都需要扫所有的事务。例如：
事务B在等事务A，事务D在等事务C，现在来了一个事务E，发现事务E需要等事务D，那么事务E就判断跟事务D、事务C是否会形成死锁，这个检测不用管事务B和事务A。
```



### 思考题

```
如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：
第一种，直接执行 delete from T limit 10000;
第二种，在一个连接中循环执行 20 次 delete from T limit 500;
第三种，在 20 个连接中同时执行 delete from T limit 500。

1.直接执行 delete from T limit 10000; 一次锁住的资源太多，事务较大，持有锁的时间最长。会导致其他客户端等待资源时间较长。当数据库压力不大的情况推荐使用这个方案。
2.推荐使用这种方案。串行化执行，将相对长的事务分成多次相对短的事务，则每次事务占用锁的时间相对较短，其他客户端在等待相应资源的时间也较短。这样的操作，同时也意味着将资源分片使用（每次执行使用不同片段的资源），可以提高并发性。
3.人为制造锁竞争，多个事务会对同一行进行产生锁的竞争，加剧并发量，更加消耗 CPU 资源。
```



### 总结

```
全局锁：对整个数据库实例加锁，flush tables with read lock; 让整个表处理只读状态。应用场景：逻辑备份。对于全部是 innodb 引擎的库，推荐使用 -single-transaction 参数，会更友好。

表锁一般是在数据库引擎不支持行锁的时候才会被用到的。MDL（元数据锁） 会直到事务提交才释放，在做表结构变更的时候，要小心不要导致锁住线上查询和更新。
【注】：MYSQL 5.6之后执行 online ddl。
执行过程：1.获取 MDL 写锁。 2.降级为 MDL 读锁。 3.执行DDL。 4.升级为 MDL 写锁。 5.释放 MDL 锁。

行锁：【注】：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。
```



## 表

### 为什么表数据删掉一半，表文件大小不变？

```
一个 InnoDB 表包含两部分，即：表结构定义和数据。在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。
```



#### 参数 innodb_file_per_table

```
表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的：

1.这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；

2.这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。

mysql5.6之后，默认为 on

一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。
```

![1677809938143](C:\Users\renyunhui\Documents\WeChat Files\wxid_r868ldaz46xv21\FileStorage\Temp\1677809938143.png)

​																												**图1 B+树索引示意图**

```
1.假设删除 R4 这个记录，Innodb 引擎只会把 R4 标记为删除，如果之后插入一个 ID 在 300 - 600之间的记录时，可能会进行复用这个位置。但是磁盘文件的大小是不会改变的。如果删除一个数据页的所有记录，那么整个数据页就可以被复用了，但是磁盘文件的大小也是不会改变的。

2.数据页的复用跟记录的复用是不同的。记录的复用仅限于符合范围条件的数据，比如图一中，R4 这条记录被删除后，如果插入一个 ID 是 400 的行，可以直接复用这个空间。但如果插入的是一个 ID 是 800 的行，就不能复用这个位置了。而当整个页从 B+ 树里面摘掉以后，可以复用到任何位置。以图 1 为例，如果将数据页 page A 上的所有记录删除以后，page A 会被标记为可复用。这时候如果要插入一条 ID = 50 的记录需要使用新页的时候，page A 是可以被复用的。

如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。

如果删除的时整个表的数据（delete），则所有的数据页都会被标记为可复用，但是在磁盘上，文件的大小是不会改变的。

【注】：delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。
```

![1677809970352](C:\Users\renyunhui\Documents\WeChat Files\wxid_r868ldaz46xv21\FileStorage\Temp\1677809970352.png)

​																												**图2 插入数据导致页分裂**

```
不止是删除数据会造成空洞，插入数据也会。

如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。

如图二所示：由于 page A 满了，再插入一个 ID 是 550 的数据时，就不得不再申请一个新的页面 page B 来保存数据了。页分裂完成后，page A 的末尾就留下了空洞（注意：实际上，可能不止 1 个记录的位置是空洞）。
同时：更新索引上的值，可以理解为删除一个旧的值，在插入一个新的值，同时也会造成空洞的。
```



#### 重建表

```
经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。而重建表，就可以达到这样的目的。
【注】：在重建表的时候，InnoDB 不会把整张表占满，每个页留了 1/16 给后续的更新用。也就是说，其实重建表之后不是“最”紧凑的。
```

![image-20230303102655121](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303102655121.png)

​																												**图3 改锁表 DDL**

```
新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。

由于表 B 是新建的表，所以表 A 主键索引上的空洞，在表 B 中就都不存在了。显然地，表 B 的主键索引更紧凑，数据页的利用率也更高。如果我们把表 B 作为临时表，数据从表 A 导入表 B 的操作完成后，用表 B 替换 A，从效果上看，就起到了收缩表 A 空间的作用。

这里，你可以使用 alter table A engine=InnoDB 命令来重建表。在 MySQL 5.5 版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表 B 不需要你自己创建，MySQL 会自动完成转存数据、交换表名、删除旧表的操作。

显然，花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。
```



![image-20230303102707454](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303102707454.png)

​																												**图4 Online DDL**

```
而在MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。重建表的大致流程：
1.建立一个临时文件，扫描表 A 主键的所有数据页；
2.用数据页中表 A 的记录生成 B+ 树，存储到临时文件中；
3.生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态；
4.临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态；
5.用临时文件替换表 A 的数据文件。

【注】：应用 row log的过程也可能产生页分裂和空洞。同时也不允许对表进行 增删改的操作。

如上图图4中：alter 语句在启动的时候需要获取 MDL 写锁，但是这个写锁在真正拷贝数据之前会退化成读锁，MDL 读锁不会阻塞其它线程对表的增删改操作,同时也是为了防止其它线程同时对这个表做 DDL 操作。

而对于一个大表来说，Online DDL 最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执行期间可以接受增删改操作。所以，相对于整个 DDL 过程来说，锁的时间非常短。对业务来说，就可以认为是 Online 的。

需要补充说明的是，上述的这些重建方法都会扫描原表数据和构建临时文件。对于很大的表来说，这个操作是很消耗 IO 和 CPU 资源的。因此，如果是线上服务，你要很小心地控制操作时间。如果想要比较安全的操作的话，推荐使用 GitHub 开源的 gh-ost 来做。
```



#### Online 和 inplace

```
在图 3 中，我们把表 A 中的数据导出来的存放位置叫作 tmp_table。这是一个临时表，是在 server 层创建的。

在图 4 中，根据表 A 重建出来的数据是放在“tmp_file”里的，这个临时文件是 InnoDB 在内部创建出来的。整个 DDL 过程都在 InnoDB 内部完成。对于 server 层来说，没有把数据挪动到临时表，是一个“原地”操作，这就是“inplace”名称的来源。

【注】：tmp_file 也是需要占用临时空间的。
```



```sql
alter table t engine = InnoDB;

-- 等价：
alter table t engine = innodb, ALGORITHM = inplace;

-- 跟 inplace 对应的就是拷贝表的方式了，用法是：
alter table t engine = innodb, ALGORITHM = copy;
```

```
当你使用 ALGORITHM = copy 的时候，表示的是强制拷贝表，对应的流程就是图 3 的操作过程。
```

```
Online 和 inplace 逻辑之间的关系可以概括为：

1.DDL 过程如果是 Online 的，就一定是 inplace 的；
2.反过来未必，也就是说 inplace 的 DDL，有可能不是 Online 的。截止到 MySQL 8.0，添加全文索引（FULLTEXT index）和空间索引 (SPATIAL index) 就属于这种情况。
```

```
延伸：optimize table、analyze table 和 alter table 这三种方式重建表的区别：
1.从 MySQL 5.6 版本开始，alter table t engine = InnoDB（也就是 recreate）默认的就是上面图 4 的流程了；
2.analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了 MDL 读锁；
3.optimize table t 等于 recreate + analyze。

【注】：tuuncate 可以理解为：drop + create
alter table 的语句会默认提交之前的所有事务，然后自己独立进行。
```



### 思考题

```
假设现在有人碰到了一个“想要收缩表空间，结果适得其反”的情况，看上去是这样的：
1.一个表 t 文件大小为 1TB；
2.对这个表执行 alter table t engine = InnoDB；
3.发现执行完成后，空间不仅没变小，还稍微大了一点儿，比如变成了 1.01TB。
你觉得可能是什么原因呢 ？

答：1.本来表的数据页就很紧凑，假设重建表之前的页利用率达到了90%以上，重新收缩的过程中页会按照90%满的比例来重新整理业数据（10%留给update使用），所以导致重新收缩之后文件反而变大了。

2.先将表 t 重建一次；然后插入一部分数据，但是插入的这些数据，用掉了一部分的预留空间；之后这种情况下，再重建一次表 t，就可能会出现文件反而变大的情况。
```



### 总结

```
1.delete 语句删除表的数据只是打了一个可复用的标记，如果是数据页上的一部分数据打了标记，对应自增主键的 insert 数据，这部分被标记可复用的数据页空间是不会被复用的，如果是整个数据页都被打上了可复用的标记，那么是可以复用的。

2.insert 语句如果数据是按照索引递增的顺序插入的，那么索引就是紧凑的，但如果数据是随机插入的，就可能造成索引的数据页分裂。

3.相邻的数据页如果存在的空洞过多，会合并成一个数据页，另一个会被打上可复用的标记。

4.mysql 5.6 之前重建表：如上图图3：新建一个与目标表结构相同的临时表，然后按照主键 ID 递增的顺序，把数据一行一行地从目标表里读出来再插入到临时表中。
由于临时表是新建的表，所以目标表主键索引上的空洞，在临时表中就都不存在了。显然地，临时表的主键索引更紧凑，数据页的利用率也更高。数据从目标表导入到临时表的操作完成后，用临时表替换目标表，从效果上看，就起到了收缩目标表空间的作用。（向临时表插入数据的时间内，是不允许对目标表进行增删改的操作的，会导致数据出现问题）。

5.mysql 5.6 之后重建表：如上图图4，可以减少空洞，文件大小可以减小。本质上是通过：创建临时文件，将表中的数据在临时文件上重建一份（用数据页中表的记录生成 B+ 树，存储到临时文件），存储的过程中按顺序的插入，极大减少了空洞的产生；生成临时文件的过程中，如果有对这个表进行增删改的操作会被记录到 row log文件中，数据拷贝完整之后将row log日志文件中的操作应用到临时文件（这个期间不可以进行对表的增删改操作），得到一个逻辑数据上与表 A 相同的数据文件，之后用临时文件替换表的数据文件。
```



## SQL

### count(*)

#### count(*) 的实现方式

```
1.MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高；(不存在 where 条件过滤的情况下)
2.InnoDB 引擎执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。
```

![image-20230303102806310](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303102806310.png)

​																						**图一：会话A，B，C的执行流程**

```
假设表 t 中现在有 10000 条记录，我们设计了三个用户并行的会话。
1.会话 A 先启动事务并查询一次表的总行数；
2.会话 B 启动事务，插入一行后记录后，查询表的总行数；
3.会话 C 先启动一个单独的语句，插入一行记录后，查询表的总行数。

即使在同一时刻的多个查询，由于多版本并发控制（MVCC）的原因，Innodb表“应该返回多少行”也是不确定的。

可重复读是 innodb 引擎默认的隔离级别，在代码上就是通过多版本并发控制，也就是 MVCC 来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于 count(*) 请求来说，InnoDB 只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。

InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。

小结：
1.MyISAM 表虽然 count(*) 很快，但是不支持事务；
2.show table status 命令虽然返回很快，但是不准确；
3.InnoDB 表直接 count(*) 会遍历全表，虽然结果准确，但会导致性能问题。
```



#### 用缓存系统保存计数？

```
问题：
1.redis异常重启，可能出现丢失更新的问题。
2.redis正常工作：
	设想一下有这么一个页面，要显示操作记录的总数，同时还要显示最近操作的 100 条记录。那么，这个页面的逻辑就需要先到 Redis 里面取出计数，再到数据表里面取数据记录。
我们是这么定义不精确的：
一种是，查到的 100 行结果里面有最新插入记录，而 Redis 的计数里还没加 1；
另一种是，查到的 100 行结果里没有最新插入的记录，而 Redis 的计数里已经加了 1。
```

![1677811058804](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811058804.png)

​																										**图 2 会话 A、B 执行时序图**

```
图 2 中，会话 A 是一个插入交易记录的逻辑，往数据表里插入一行 R，然后 Redis 计数加 1；会话 B 就是查询页面显示时需要的数据。

在图 2 的这个时序里，在 T3 时刻会话 B 来查询的时候，会显示出新插入的 R 这个记录，但是 Redis 的计数还没加 1。这时候，就会出现我们说的数据不一致。

你一定会说，这是因为我们执行新增记录逻辑时候，是先写数据表，再改 Redis 计数。而读的时候是先读 Redis，再读数据表，这个顺序是相反的。那么，如果保持顺序一样的话，是不是就没问题了？我们现在把会话 A 的更新顺序换一下，再看看执行结果。
```

![1677811090515](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811090515.png)

​																					**图 3 调整顺序后，会话 A、B 的执行时序图**

```
这时候反过来了，会话 B 在 T3 时刻查询的时候，Redis 计数加了 1 了，但还查不到新插入的 R 这一行，也是数据不一致的情况。

在并发系统里面，我们是无法精确控制不同线程的执行时刻的，因为存在图中的这种操作序列，所以，我们说即使 Redis 正常工作，这个计数值还是逻辑上不精确的。
```



#### 在数据库保存计数

```
把这个计数直接放到数据库里单独的一张计数表 C 中,首先可以解决崩溃丢失的问题，Innodb支持崩溃恢复不丢失数据的。
```

![1677811103873](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811103873.png)

​																							**图 4 会话 A、B 的执行时序图**

```
虽然会话 B 的读操作仍然是在 T3 执行的，但是因为这时候更新事务还没有提交，所以计数值加 1 这个操作对会话 B 还不可见。

因此，会话 B 看到的结果里， 查计数值和“最近 100 条记录”看到的结果，逻辑上就是一致的。
```



#### 不同的 count 用法

```
count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。
所以，count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数；而 count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。

至于分析性能差别的时候，你可以记住这么几个原则：
1.server 层要什么就给什么；
2.InnoDB 只给必要的值；
3.现在的优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做。


对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表（可能会选择最小的索引进行遍历），把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。
对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。
单看这两个用法的差别的话，你能对比出来，count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。
对于 count(字段) 来说：（如果字段上没有索引则会走主键索引）
如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；
如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。
也就是前面的第一条原则，server 层要什么字段，InnoDB 就返回什么字段。
但是 count(*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。
看到这里，你一定会说，优化器就不能自己判断一下吗，主键 id 肯定非空啊，为什么不能按照 count(*) 来处理，多么简单的优化啊。
当然，MySQL 专门针对这个语句进行优化，也不是不可以。但是这种需要专门优化的情况太多了，而且 MySQL 已经优化过 count(*) 了，你直接使用这种用法就可以了。
所以结论是：按照效率排序的话，count(字段)<count(主键 id)<count(1)≈count(*)，所以我建议你，尽量使用 count(*)。
```



### "order by"

```sql
CREATE TABLE `ttt` (
  `id` int(11) NOT NULL,
  `city` varchar(16) NOT NULL,
  `name` varchar(16) NOT NULL,
  `age` int(11) NOT NULL,
  `addr` varchar(128) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `city` (`city`)
) ENGINE=InnoDB;

-- 假设你要查询城市是“杭州”的所有人名字，并且按照姓名排序返回前 1000 个人的姓名、年龄。
select city,name,age from ttt where city='杭州' order by name limit 1000;
```



#### 全字段排序

![1677811245669](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811245669.png)

​																						**图 1 使用 explain 命令查看语句的执行情况**

```
图1中：字段Extra中的“Using filesort"，表示需要排序，Mysql 会给每个线程分配一块内存用于排序，”sort buffer“。
```

![image-20220113154125236](C:\Users\renyunhui\Documents\WeChat Files\wxid_r868ldaz46xv21\FileStorage\Temp\1677811254301.png)

​																										**图 2 city 字段的索引示意图**

```
执行流程：
1.初始化 sort_buffer，确定放入 city name age三个字段；
2.从索引 city 中找到第一个满足 “city = '杭州'" 条件的主键ID；
3.去主键 id 索引树取出整行的数据，取出city name age三个字段，存入 sort_buffer 中；
4.再去 city 索引树取出下一个记录的主键id；
5.重复3,4的操作，直到 city 的值不满足查询条件为止；
6.对 sort_buffer 中的数据按照字段 name 做快速排序；
7.按照排序结果取前 1000 行返回给客户端。
```

![1677811317520](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811317520.png)

​																													**图 3 全字段排序**

```
图3中：按name 字段进行排序，可能是在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。
sort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。
```

```sql
-- 确定一个排序语句是否使用了临时文件
/* 打开 optimizer_trace，只对本线程有效 */
SET optimizer_trace='enabled=on'; 
 
/* @a 保存 Innodb_rows_read 的初始值 */
select VARIABLE_VALUE into @a from  performance_schema.session_status where variable_name = 'Innodb_rows_read';
 
/* 执行语句 */
select city, name,age from t where city='杭州' order by name limit 1000; 
 
/* 查看 OPTIMIZER_TRACE 输出 */
SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G
 
/* @b 保存 Innodb_rows_read 的当前值 */
select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';
 
/* 计算 Innodb_rows_read 差值 */
select @b-@a;
```

![1677811351922](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811351922.png)

​																				**图 4 全排序的 OPTIMIZER_TRACE 部分结果**

```
number_of_tmp_files 表示的是，排序过程中使用的临时文件数。内存放不下时，就需要使用外部排序，外部排序一般使用归并排序算法。可以这么简单理解，MySQL 将需要排序的数据分成 12 份，每一份单独排序后存在这些临时文件中。然后把这 12 个有序文件再合并成一个有序的大文件。

如果 sort_buffer_size 超过了需要排序的数据量的大小，number_of_tmp_files 就是 0，表示排序可以直接在内存中完成。

否则就需要放在临时文件中排序。sort_buffer_size 越小，需要分成的份数越多，number_of_tmp_files 的值就越大。

示例表中有 4000 条满足 city='杭州’的记录，所以你可以看到 examined_rows=4000，表示参与排序的行数是 4000 行。

sort_mode 里面的 packed_additional_fields 的意思是，排序过程对字符串做了“紧凑”处理。即使 name 字段的定义是 varchar(16)，在排序过程中还是要按照实际长度来分配空间的。

同时，最后一个查询语句 select @b-@a 的返回结果是 4000，表示整个执行过程只扫描了 4000 行。

这里需要注意的是，为了避免对结论造成干扰，我把 internal_tmp_disk_storage_engine 设置成 MyISAM。否则，select @b-@a 的结果会显示为 4001。

这是因为查询 OPTIMIZER_TRACE 这个表时，需要用到临时表，而 internal_tmp_disk_storage_engine 的默认值是 InnoDB。如果使用的是 InnoDB 引擎的话，把数据从临时表取出来的时候，会让 Innodb_rows_read 的值加 1。
```



#### rowid排序

```
全字段排序，只对原表的数据读了一遍，剩下的操作都是在 sort_buffer 和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。

所以如果单行很大，这个方法效率不够好。

那么，如果 MySQL 认为排序的单行长度太大会怎么做?
```

```sql
SET max_length_for_sort_data = 16;

-- max_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。
```

![1677811380208](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811380208.png)

​																										**图 5 rowid 排序**

```
city、name、age 这三个字段的定义总长度是 36，我把 max_length_for_sort_data 设置为 16，看看计算过程有什么改变。

新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。
但这时，排序的结果就因为少了 city 和 age 字段的值，不能直接返回了，整个执行流程:
1.初始化 sort_buffer，确定放入两个字段，即 name 和 id；
2.从索引 city 找到第一个满足 city='杭州’条件的主键 id，也就是图中的 ID_X；
3.到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中；
4.从索引 city 取下一个记录的主键 id；
5.重复步骤 3、4 直到不满足 city='杭州’条件为止，也就是图中的 ID_Y；
6.对 sort_buffer 中的数据按照字段 name 进行排序；
7.遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。
```

![1677811443353](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811443353.png)

​																					**图 6 rowid 排序的 OPTIMIZER_TRACE 部分输出**

```
对比图 3 的全字段排序流程图你会发现，rowid 排序多访问了一次表 t 的主键索引，就是步骤 7。
需要说明的是，最后的“结果集”是一个逻辑概念，实际上 MySQL 服务端从排序后的 sort_buffer 中依次取出 id，然后到原表查到 city、name 和 age 这三个字段的结果，不需要在服务端再耗费内存存储结果，是直接返回给客户端的。
根据这个说明过程和图示，你可以想一下，这个时候执行 select @b-@a，结果会是多少呢？
现在，我们就来看看结果有什么不同。
首先，图中的 examined_rows 的值还是 4000，表示用于排序的数据是 4000 行。但是 select @b-@a 这个语句的值变成 5000 了。
因为这时候除了排序过程外，在排序完成后，还要根据 id 去原表取值。由于语句是 limit 1000，因此会多读 1000 行。

从 OPTIMIZER_TRACE 的结果中，你还能看到另外两个信息也变了。
1.sort_mode 变成了 <sort_key, rowid>，表示参与排序的只有 name 和 id 这两个字段。
2.number_of_tmp_files 变成 10 了，是因为这时候参与排序的行数虽然仍然是 4000 行，但是每一行都变小了，因此需要排序的总数据量就变小了，需要的临时文件也相应地变少了。
```



#### 全字段排序 VS rowid 排序

```
1.如果 MySQL 担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。

2.如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。

这也就体现了 MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。

【注】：并不是所有的 order by 语句，都需要排序操作的。从上面分析的执行过程，我们可以看到，MySQL 之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。
```

```sql
-- 在这个市民表上创建一个 city 和 name 的联合索引
alter table t add index city_user(city, name);
```

![1677811482073](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811482073.png)

​																					**图 7 city 和 name 联合索引示意图**

![1677811494623](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811494623.png)

​																	**图 8 引入 (city,name) 联合索引后，查询语句的执行计划**

```
在这个索引里面，我们依然可以用树搜索的方式定位到第一个满足 city='杭州’的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要 city 的值是杭州，name 的值就一定是有序的。

流程：
1.从索引 (city,name) 找到第一个满足 city='杭州’条件的主键 id；
2.到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回；
3.从索引 (city,name) 取下一个记录主键 id；
4.重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city='杭州’条件时循环结束。

这个查询过程不需要临时表，也不需要排序
```

![1677811518517](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811518517.png)

​															**图 9 引入 (city,name) 联合索引后，查询语句的执行计划**

```
从图中可以看到，Extra 字段中没有 Using filesort 了，也就是不需要排序了。而且由于 (city,name) 这个联合索引本身有序，所以这个查询也不用把 4000 行全都读一遍，只要找到满足条件的前 1000 条记录就可以退出了。也就是说，在我们这个例子里，只需要扫描 1000 次。

进一步优化：
使用覆盖索引，如果索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。
```

```sql
alter table t add index city_user_age(city, name, age);
```

```
流程：
对于 city 字段的值相同的行来说，还是按照 name 字段的值递增排序的，此时的查询语句也就不再需要排序了。这样整个查询语句的执行流程就变成了：
1.从索引 (city,name,age) 找到第一个满足 city='杭州’条件的记录，取出其中的 city、name 和 age 这三个字段的值，作为结果集的一部分直接返回；
2.从索引 (city,name,age) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回；
3.重复执行步骤 2，直到查到第 1000 条记录，或者是不满足 city='杭州’条件时循环结束。
```

![1677811531236](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811531236.png)

​																		**图 10 引入 (city,name,age) 联合索引后，查询语句的执行流程**

![1677811541067](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677811541067.png)

​																**图 11 引入 (city,name,age) 联合索引后，查询语句的执行计划**

```
Extra 字段里面多了“Using index”，表示的就是使用了覆盖索引，性能上会快很多。
```



### 随机结果

```sql
CREATE TABLE `words` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `word` varchar(64) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
 
delimiter ;;
create procedure idata()
begin
  declare i int;
  set i=0;
  while i<10000 do
    insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10))));
    set i=i+1;
  end while;
end;;
delimiter ;
 
call idata();
```



#### 内存临时表

```sql
select word from words order by rand() limit 3;
```

![image-20220114133833192](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220114133833192.png)

​																					**图 1 使用 explain 命令查看语句的执行情况**

```
图一：Extra字段显示：Using temporary，表示的是需要使用临时表；Using filesort，表示的是需要执行排序操作。（需要在临时表进行排序）

对于临时内存表的排序来说，它会选择哪一种算法呢？
1.对于 InnoDB 表来说，执行全字段排序会减少磁盘访问，因此会被优先选择。
2.对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。优化器没有了这一层顾虑，那么它会优先考虑的，就是用于排序的行越小越好了，所以，MySQL 这时就会选择 rowid 排序。
```

![image-20220114134020574](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220114134020574.png)

​																									**图 2 随机排序完整流程图 1**

```
语句的执行流程:
1.创建一个临时表。这个临时表使用的是 memory 引擎，表里有两个字段，第一个字段是 double 类型，记为字段 R，第二个字段是 varchar(64) 类型，记为字段 W。并且，这个表没有建索引。
2.从 words 表中，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand() 函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。
3.临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序。
4.初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型。
5.从内存临时表中一行一行地取出 R 值和位置信息（我后面会和你解释这里为什么是“位置信息”），分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000。
6.在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数。
7.排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。

图2中的 pos 就是位置信息。
MySQL 的表是用什么方法来定位“一行数据”的？
如果你创建的表没有主键，或者把一个表的主键删掉了，那么 InnoDB 会自己生成一个长度为 6 字节的 rowid 来作为主键。
这也就是排序模式里面，rowid 名字的来历。实际上它表示的是：每个引擎用来唯一标识数据行的信息。
1.对于有主键的 InnoDB 表来说，这个 rowid 就是主键 ID；
2.对于没有主键的 InnoDB 表来说，这个 rowid 就是由系统生成的；
3.MEMORY 引擎不是索引组织表。在这个例子里面，你可以认为它就是一个数组。因此，这个 rowid 其实就是数组的下标。

小结：order by rand() 使用了内存临时表，内存临时表排序的时候使用了 rowid 排序方法。
```



#### 磁盘临时表

```
其实不是所有的临时表都是内存表呢？。tmp_table_size 这个配置限制了内存临时表的大小，默认值是 16M。如果临时表大小超过了 tmp_table_size，那么内存临时表就会转成磁盘临时表。

磁盘临时表使用的引擎默认是 InnoDB，是由参数 internal_tmp_disk_storage_engine 控制的。

当使用磁盘临时表的时候，对应的就是一个没有显式索引的 InnoDB 表的排序过程。
```

```sql
set tmp_table_size=1024;
set sort_buffer_size=32768;
set max_length_for_sort_data=16;
/* 打开 optimizer_trace，只对本线程有效 */
SET optimizer_trace='enabled=on'; 
 
/* 执行语句 */
select word from words order by rand() limit 3;
 
/* 查看 OPTIMIZER_TRACE 输出 */
SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G
```

![image-20220114140518935](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220114140518935.png)

​																		**图 3 OPTIMIZER_TRACE 部分结果**

```
这次 OPTIMIZER_TRACE 的结果如上图所示。

因为将 max_length_for_sort_data 设置成 16，小于 word 字段的长度定义，所以我们看到 sort_mode 里面显示的是 rowid 排序，这个是符合预期的，参与排序的是随机值 R 字段和 rowid 字段组成的行。

这时候你可能心算了一下，发现不对。R 字段存放的随机值就 8 个字节，rowid 是 6 个字节（至于为什么是 6 字节，就留给你课后思考吧），数据总行数是 10000，这样算出来就有 140000 字节，超过了 sort_buffer_size 定义的 32768 字节了。但是，number_of_tmp_files 的值居然是 0，难道不需要用临时文件吗？

这个 SQL 语句的排序确实没有用到临时文件，采用是 MySQL 5.6 版本引入的一个新的排序算法，即：优先队列排序算法。接下来，我们就看看为什么没有使用临时文件的算法，也就是归并排序算法，而是采用了优先队列排序算法。

其实，我们现在的 SQL 语句，只需要取 R 值最小的 3 个 rowid。但是，如果使用归并排序算法的话，虽然最终也能得到前 3 个值，但是这个算法结束后，已经将 1000 行数据都排好序了。

也就是说，后面的 9997 行也是有序的了。但，我们的查询并不需要这些数据是有序的。所以，想一下就明白了，这浪费了非常多的计算量。
```

```
优先队列算法，就可以精确地只得到三个最小值，执行流程如下：

1.对于这 10000 个准备排序的 (R,rowid)，先取前三行，构造成一个堆；
（对数据结构印象模糊的同学，可以先设想成这是一个由三个元素组成的数组）
2.取下一个行 (R’,rowid’)，跟当前堆里面最大的 R 比较，如果 R’小于 R，把这个 (R,rowid) 从堆中去掉，换成 (R’,rowid’)；
3.重复第 2 步，直到第 10000 个 (R’,rowid’) 完成比较。
```

![图 6 优先队列排序算法示例](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/图 6 优先队列排序算法示例.png)

​																							**图 4 优先队列排序算法示例**

```
图 4 是模拟 6 个 (R,rowid) 行，通过优先队列排序找到最小的三个 R 值的行的过程。整个排序过程中，为了最快地拿到当前堆的最大值，总是保持最大值在堆顶，因此这是一个最大堆。

图 3 的 OPTIMIZER_TRACE 结果中，filesort_priority_queue_optimization 这个部分的 chosen=true，就表示使用了优先队列排序算法，这个过程不需要临时文件，因此对应的 number_of_tmp_files 是 0。

这个流程结束后，我们构造的堆里面，就是这个 10000 行里面 R 值最小的三行。然后，依次把它们的 rowid 取出来，去临时表里面拿到 word 字段，这个过程就跟之前的 rowid 排序的过程一样了。
```



#### 随机排序方法

##### 随机算法一

```
如果只随机选择 1 个 word 值，可以怎么做呢？思路上是这样的：
1.取得这个表的主键 id 的最大值 M 和最小值 N;
2.用随机函数生成一个最大值到最小值之间的数 X = (M-N)*rand() + N;
3.取不小于 X 的第一个 ID 的行。
我们把这个算法，暂时称作随机算法 1
```

```sql
select max(id),min(id) into @M,@N from t ;
set @X= floor((@M-@N+1)*rand() + @N);
select * from t where id >= @X limit 1;
```

```
这个方法效率很高，因为取 max(id) 和 min(id) 都是不需要扫描索引的，而第三步的 select 也可以用索引快速定位，可以认为就只扫描了 3 行。但实际上，这个算法本身并不严格满足题目的随机要求，因为 ID 中间可能有空洞，因此选择不同行的概率不一样，不是真正的随机。

比如你有 4 个 id，分别是 1、2、4、5，如果按照上面的方法，那么取到 id=4 的这一行的概率是取得其他行概率的两倍。

如果这四行的 id 分别是 1、2、40000、40001 呢？这个算法基本就能当 bug 来看待了。
```



##### 随机算法二

```
为了得到严格随机的结果，你可以用下面这个流程:
1.取得整个表的行数，并记为 C。
2.取得 Y = floor(C * rand())。 floor 函数在这里的作用，就是取整数部分。
3.再用 limit Y,1 取得一行。
我们把这个算法，称为随机算法 2。下面这段代码，就是上面流程的执行语句的序列。
```

```sql
select count(*) into @C from t;
set @Y = floor(@C * rand());
set @sql = concat("select * from t limit ", @Y, ",1");
prepare stmt from @sql;
execute stmt;
DEALLOCATE prepare stmt;
```

```
由于 limit 后面的参数不能直接跟变量，所以我在上面的代码中使用了 prepare+execute 的方法。你也可以把拼接 SQL 语句的方法写在应用程序中，会更简单些。

这个随机算法 2，解决了算法 1 里面明显的概率不均匀问题。

MySQL 处理 limit Y,1 的做法就是按顺序一个一个地读出来，丢掉前 Y 个，然后把下一个记录作为返回结果，因此这一步需要扫描 Y+1 行。再加上，第一步扫描的 C 行，总共需要扫描 C+Y+1 行，执行代价比随机算法 1 的代价要高。

当然，随机算法 2 跟直接 order by rand() 比起来，执行代价还是小很多的。

如果按照这个表有 10000 行来计算的话，C=10000，要是随机到比较大的 Y 值，那扫描行数也跟 20000 差不多了，接近 order by rand() 的扫描行数，为什么说随机算法 2 的代价要小很多呢？
```



##### 随机算法三

```
要随机取 3 个 word 值呢？你可以这么做：
1.取得整个表的行数，记为 C；
2.根据相同的随机方法得到 Y1、Y2、Y3；
3.再执行三个 limit Y, 1 语句得到三行数据。
我们把这个算法，称作随机算法 3。下面这段代码，就是上面流程的执行语句的序列。
```

```sql
select count(*) into @C from t;
set @Y1 = floor(@C * rand());
set @Y2 = floor(@C * rand());
set @Y3 = floor(@C * rand());
select * from t limit @Y1，1； // 在应用代码里面取 Y1、Y2、Y3 值，拼出 SQL 后执行
select * from t limit @Y2，1；
select * from t limit @Y3，1；
```



### SQL语句逻辑相同，性能却差异巨大？

#### 案例一：条件字段函数操作

```sql
-- 假设你现在维护了一个交易系统，其中交易记录表 tradelog 包含交易流水号（tradeid）、交易员 id（operator）、交易时间（t_modified）等字段。为了便于描述，我们先忽略其他字段。这个表的建表语句如下：

CREATE TABLE `tradelog` (
  `id` int(11) NOT NULL,
  `tradeid` varchar(32) DEFAULT NULL,
  `operator` int(11) DEFAULT NULL,
  `t_modified` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `tradeid` (`tradeid`),
  KEY `t_modified` (`t_modified`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

-- 假设，现在已经记录了从 2016 年初到 2018 年底的所有数据，运营部门有一个需求是，要统计发生在所有年份中 7 月份的交易记录总数。这个逻辑看上去并不复杂，你的 SQL 语句可能会这么写：
select count(*) from tradelog where month(t_modified)=7;
```

![image-20230303105306934](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303105306934.png)

​																										**图 1 t_modified 索引示意图**

```
如果你的 SQL 语句条件用的是 where t_modified='2018-7-1’的话，引擎就会按照上面绿色箭头的路线，快速定位到 t_modified='2018-7-1’需要的结果。
实际上，B+ 树提供的这个快速定位能力，来源于同一层兄弟节点的有序性。
但是，如果计算 month() 函数的话，你会看到传入 7 的时候，在树的第一层就不知道该怎么办了。
也就是说，对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。

需要注意的是，优化器并不是要放弃使用这个索引。
在这个例子里，放弃了树搜索功能，优化器可以选择遍历主键索引，也可以选择遍历索引 t_modified，优化器对比索引大小后发现，索引 t_modified 更小，遍历这个索引比遍历主键索引来得更快。因此最终还是会选择索引 t_modified。
```

![image-20230303105317209](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303105317209.png)

​																											**图 2 explain 结果**

```
key：t_modified 说明使用了 t_modified 索引。这条语句扫描了整个索引的所有值；Extra 字段的 Using index，表示的是使用了覆盖索引。
当前数据库的存在10条记录，rows = 10，说明走了全表的扫描。

也就是说，由于在 t_modified 字段加了 month() 函数操作，导致全索引扫描。为了能用上索引的快速定位能力，我们就要把 SQL 语句改成基于字段本身的范围查询。
```

```sql
select count(*) from tradelog where
    (t_modified >= '2016-7-1' and t_modified<'2016-8-1') or
    (t_modified >= '2017-7-1' and t_modified<'2017-8-1') or 
    (t_modified >= '2018-7-1' and t_modified<'2018-8-1');
```

```
【注】：加了 month() 函数操作，MySQL 无法再使用索引快速定位功能，而只能使用全索引扫描。
优化器在个问题上确实有“偷懒”行为，即使是对于不改变有序性的函数，也不会考虑使用索引
例：对于 select * from tradelog where id + 1 = 10000 这个 SQL 语句，这个加 1 操作并不会改变有序性，但是 MySQL 优化器还是不能用 id 索引快速定位到 9999 这一行。所以，需要你在写 SQL 语句的时候，手动改写成 where id = 10000 -1 才可以。
```



#### 案例二：隐式类型转换

```sql
select * from tradelog where tradeid=110717;
```

```
交易编号 tradeid 这个字段上，本来就有索引，但是 explain 的结果却显示，这条语句需要走全表扫描。
原因：tradeid 的字段类型是 varchar(32)，而输入的参数却是整型，所以需要做类型转换。
```

```
问题1：数据类型转换的规则是什么？
问题2：为什么有数据类型转换，就需要走全索引扫描？

在 MySQL 中，字符串和数字做比较的话，是将字符串转换成数字。
导致全表扫描的语句：select * from tradelog where tradeid=110717;
对于优化器而言：select * from tradelog where  CAST(tradid AS signed int) = 110717;（将字符串类型的字段值转为数字类型，对于字段进行了函数的操作，优化器会放弃走树搜索功能，从而导致全表扫描）。
```



#### 案例三：隐式字符编码转换

```sql
-- 假设系统里还有另外一个表 trade_detail，用于记录交易的操作细节。为了便于量化分析和复现，我往交易日志表 tradelog 和交易详情表 trade_detail 这两个表里插入一些数据。

CREATE TABLE `trade_detail` (
  `id` int(11) NOT NULL,
  `tradeid` varchar(32) DEFAULT NULL,
  `trade_step` int(11) DEFAULT NULL, /* 操作步骤 */
  `step_info` varchar(32) DEFAULT NULL, /* 步骤信息 */
  PRIMARY KEY (`id`),
  KEY `tradeid` (`tradeid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
 
insert into tradelog values(11, 'aaaaaaaa', 1000, now());
insert into tradelog values(12, 'aaaaaaab', 1000, now());
insert into tradelog values(13, 'aaaaaaac', 1000, now());
 
insert into trade_detail values(11, 'aaaaaaaa', 1, 'add');
insert into trade_detail values(12, 'aaaaaaaa', 2, 'update');
insert into trade_detail values(13, 'aaaaaaaa', 3, 'commit');
insert into trade_detail values(14, 'aaaaaaab', 1, 'add');
insert into trade_detail values(15, 'aaaaaaab', 2, 'update');
insert into trade_detail values(16, 'aaaaaaab', 3, 'update again');
insert into trade_detail values(17, 'aaaaaaab', 4, 'commit');
insert into trade_detail values(18, 'aaaaaaac', 1, 'add');
insert into trade_detail values(19, 'aaaaaaac', 2, 'update');
insert into trade_detail values(110, 'aaaaaaac', 3, 'update again');
insert into trade_detail values(111, 'aaaaaaac', 4, 'commit');

-- 查询 id=2 的交易的所有操作步骤信息
select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; /* 语句 Q1*/
```

![image-20230303105915588](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303105915588.png)

​																							**图 4 语句 Q1 的 explain 结果**

```
1.第一行显示优化器会先在交易记录表 tradelog 上查到 id=2 的行，这个步骤用上了主键索引，rows=1 表示只扫描一行；
2.第二行 key=NULL，表示没有用上交易详情表 trade_detail 上的 tradeid 索引，进行了全表扫描。
```

![image-20230303105924942](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303105924942.png)

​																										**图 5 语句 Q1 的执行过程**

```
在这个执行计划里，是从 tradelog 表中取 tradeid 字段，再去 trade_detail 表里查询匹配字段。因此，我们把 tradelog 称为驱动表，把 trade_detail 称为被驱动表，把 tradeid 称为关联字段。explain 结果表示的执行流程如上图5所示。

1.第 1 步，是根据 id 在 tradelog 表里找到 L2 这一行；
2.第 2 步，是从 L2 中取出 tradeid 字段的值；
3.第 3 步，是根据 tradeid 值到 trade_detail 表中查找条件匹配的行。explain 的结果里面第二行的 key=NULL 表示的就是，这个过程是通过遍历主键索引的方式，一个一个地判断 tradeid 的值是否匹配。

疑问：第 3 步不符合我们的预期。因为表 trade_detail 里 tradeid 字段上是有索引的，我们本来是希望通过使用 tradeid 索引能够快速定位到等值的行。但，这里并没有。
答：因为这两个表的字符集不同，一个是 utf8，一个是 utf8mb4，所以做表连接查询的时候用不上关联字段的索引。
问题是出在执行步骤的第 3 步，如果单独把这一步改成 SQL 语句的话，那就是：
select * from trade_detail where tradeid=$L2.tradeid.value; 
其中，$L2.tradeid.value 的字符集是 utf8mb4。
```

```
字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。

这个设定很好理解，utf8mb4 是 utf8 的超集。类似地，在程序设计语言里面，做自动类型转换的时候，为了避免数据在转换过程中由于截断导致数据错误，也都是“按数据长度增加的方向”进行转换的。

因此， 在执行上面这个语句的时候，需要将被驱动数据表里的字段一个个地转换成 utf8mb4，再跟 L2 做比较。

也就是说，实际上这个语句等同于下面这个写法：
```

```sql
select * from trade_detail  where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value; 
```

```
结论：CONVERT() 函数，在这里的意思是把输入的字符串转成 utf8mb4 字符集。

这就再次触发了我们上面说到的原则：对索引字段做函数操作，优化器会放弃走树搜索功能。

字符集不同只是条件之一，连接过程中要求在被驱动表的索引字段上加函数操作，是直接导致对被驱动表做全表扫描的原因。
```

```sql
-- “查找 trade_detail 表里 id=4 的操作，对应的操作者是谁”，再来看下这个语句和它的执行计划。
select l.operator from tradelog l , trade_detail d where d.tradeid=l.tradeid and d.id=4;
```

![image-20230303105939095](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303105939095.png)

​																											**图 6 explain 结果**

```
这个语句里 trade_detail 表成了驱动表，但是 explain 结果的第二行显示，这次的查询操作用上了被驱动表 tradelog 里的索引 (tradeid)，扫描行数是 1。

这也是两个 tradeid 字段的 join 操作，为什么这次能用上被驱动表的 tradeid 索引呢？
```

```sql
-- 假设驱动表 trade_detail 里 id=4 的行记为 R4，那么在连接的时候（图 5 的第 3 步），被驱动表 tradelog 上执行的就是类似这样的 SQL 语句：
select operator from tradelog  where traideid =$R4.tradeid.value; 

-- 这时候 $R4.tradeid.value 的字符集是 utf8, 按照字符集转换规则，要转成 utf8mb4，所以这个过程就被改写成：
select operator from tradelog  where traideid =CONVERT($R4.tradeid.value USING utf8mb4); 

-- 这里的 CONVERT 函数是加在输入参数上的，这样就可以用上被驱动表的 traideid 索引。
-- 优化前：
select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2;

-- 优化后1：把 trade_detail 表上的 tradeid 字段的字符集也改成 utf8mb4，这样就没有字符集转换的问题了。
alter table trade_detail modify tradeid varchar(32) CHARACTER SET utf8mb4 default null;

-- 优化2：但如果数据量比较大， 或者业务上暂时不能做这个 DDL 的话，那就只能采用修改 SQL 语句的方法了。
-- 主动把 l.tradeid 转成 utf8，就避免了被驱动表上的字符编码转换，从 explain 结果可以看到，这次索引走对了。
select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; 
```

![image-20230303105949405](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303105949405.png)

​																							**图 7 SQL 语句优化2后的 explain 结果**



**小结：对目标索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。索引字段不能进行函数操作，但是索引字段的参数可以做函数操作**



## 只查一行的语句，也执行这么慢？

```sql
CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
 
delimiter ;;
create procedure idata()
begin
  declare i int;
  set i=1;
  while(i<=100000)do
    insert into t values(i,i);
    set i=i+1;
  end while;
end;;
delimiter ;
 
call idata();
```



#### 第一类：查询长时间不返回

```sql
select * from t where id=1;

-- 如果查询结果长时间不返回，通过下面的sql，查看当前语句处于什么状态，以及如何处理。
show processlist;
```

![image-20230303110120947](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303110120947.png)

​																			**图 1 Waiting for table metadata lock 状态示意图**

```
如图 1 所示，就是使用 show processlist 命令查看 Waiting for table metadata lock 的示意图。

这个状态表示的是，现在有一个线程正在表 t 上请求或者持有 MDL 写锁，把 select 语句堵住了。
```

![image-20230303110133126](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303110133126.png)

​															**图 2 MySQL  中 Waiting for table metadata lock 的复现步骤**

```
session A 通过 lock table 命令持有表 t 的 MDL 写锁，而 session B 的查询需要获取 MDL 读锁。所以，session B 进入等待状态。

这类问题的处理方式，就是找到谁持有 MDL 写锁，然后把它 kill 掉。

但是，由于在 show processlist 的结果里面，session A 的 Command 列是“Sleep”，导致查找起来很不方便。不过有了 performance_schema 和 sys 系统库以后，就方便多了。（MySQL 启动时需要设置 performance_schema=on，相比于设置为 off 会有 10% 左右的性能损失)

通过查询 sys.schema_table_lock_waits 这张表，我们就可以直接找出造成阻塞的 process id，把这个连接用 kill 命令断开即可。
```



##### 等待 flush

![image-20230303110153558](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303110153558.png)

​																					**图 3 Waiting for table flush 的复现步骤**

```
在 session A 中，我故意每行都调用一次 sleep(1)，这样这个语句默认要执行 10 万秒，在这期间表 t 一直是被 session A“打开”着。然后，session B 的 flush tables t 命令再要去关闭表 t，就需要等 session A 的查询结束。这样，session C 要再次查询的话，就会被 flush 命令堵住了。

出现 Waiting for table flush 状态的可能情况是：有一个 flush tables 命令被别的语句堵住了，然后它又堵住了我们的 select 语句。
```

![image-20230303110203804](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303110203804.png)

​																				**图 4 Waiting for table flush 状态示意图**

![image-20230303110210608](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303110210608.png)

​																				**图 5 Waiting for table flush 的 show processlist 结果**



##### 等行锁

![image-20230303110220995](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303110220995.png)

​																										**图 6 行锁复现**

![image-20230303110227757](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303110227757.png)

​																								**图 7 行锁 show processlist 现场**



```
显然，session A 启动了事务，占有写锁，还不提交，是导致 session B 被堵住的原因。
```

```sql
-- 通过下面的sql查询是谁占用了写锁：
SELECT * FROM `performance_schema`.data_lock_waits;

-- 干掉对应的线程即可
```



#### 第二类：查询慢

![image-20230303110253132](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303110253132.png)

​																													**图 8 复现步骤**

![image-20230303110307956](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303110307956.png)

​																										**图 9 id=1 的数据状态**

```
带 lock in share mode 的 SQL 语句，是当前读，因此会直接读到 1000001 这个结果，所以速度很快；而 select * from t where id=1 这个语句，是一致性读，因此需要从 1000001 开始，依次执行 undo log，执行了 100 万次以后，才将 1 这个结果返回。

【注】：undo log 里记录的其实是“把 2 改成 1”，“把 3 改成 2”这样的操作逻辑。
```



### 幻读

#### 什么是幻读？

如下图所示：前提假设只在 id = 5 这一行加行锁

![image-20230303111016274](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303111016274.png)

​																						    	**图 1 假设只在 id=5 这一行加行锁**

```
session A 里执行了三次查询，分别是 Q1、Q2 和 Q3。它们的 SQL 语句相同，都是 select * from t where d=5 for update。这个语句的意思你应该很清楚了，查所有 d=5 的行，而且使用的是当前读（select ... for update），并且加上写锁。
1.Q1 只返回 id=5 这一行；
2.在 T2 时刻，session B 把 id=0 这一行的 d 值改成了 5，因此 T3 时刻 Q2 查出来的是 id=0 和 id=5 这两行；
3.在 T4 时刻，session C 又插入一行（1,1,5），因此 T5 时刻 Q3 查出来的是 id=0、id=1 和 id=5 的这三行。

其中，Q3 读到 id=1 这一行的现象，被称为“幻读”。也就是说，幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。

幻读：
1.在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。
2.上面 session B 的修改结果，被 session A 之后的 select 语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。
```



#### 幻读存在的问题

##### 语义方面

```
1.语义上面：session A 在 T1 时刻就声明了，“我要把所有 d=5 的行锁住，不准别的事务进行读写操作”。而实际上，这个语义被破坏了。
```

![image-20230303111120824](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230303111120824.png)

​																					**图 2 假设只在 id=5 这一行加行锁 -- 语义被破坏**

```
session B 的第二条语句 update t set c=5 where id=0，语义是“我把 id=0、d=5 这一行的 c 值，改成了 5”。

由于在 T1 时刻，session A 还只是给 id=5 这一行加了行锁， 并没有给 id=0 这行加上锁。因此，session B 在 T2 时刻，是可以执行这两条 update 语句的。这样，就破坏了 session A 里 Q1 语句要锁住所有 d=5 的行的加锁声明。

session C 也是一样的道理，对 id=1 这一行的修改，也是破坏了 Q1 的加锁声明。
```



##### 数据一致性方面

```
锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。

为了说明这个问题，我给 session A 在 T1 时刻再加一个更新语句，即：update t set d=100 where d=5。
```

![1677813128940](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677813128940.png)

​																				**图 3 假设只在 id=5 这一行加行锁 -- 数据一致性问题**

```
update 的加锁语义和 select …for update 是一致的，所以这时候加上这条 update 语句也很合理。session A 声明说“要给 d=5 的语句加上锁”，就是为了要更新数据，新加的这条 update 语句就是把它认为加上了锁的这一行的 d 值修改成了 100。

分析一下图 3 执行完成后，数据库里会是什么结果。
1.经过 T1 时刻，id=5 这一行变成 (5,5,100)，当然这个结果最终是在 T6 时刻正式提交的 ;
2.经过 T2 时刻，id=0 这一行变成 (0,5,5);
3.经过 T4 时刻，表里面多了一行 (1,5,5);

其他行跟这个执行序列无关，保持不变。

这时候 binlog 里面的内容。
1.T2 时刻，session B 事务提交，写入了两条语句；
2.T4 时刻，session C 事务提交，写入了两条语句；
3.T6 时刻，session A 事务提交，写入了 update t set d=100 where d=5 这条语句。
```

```sql
-- binlog
update t set d=5 where id=0; /*(0,0,5)*/
update t set c=5 where id=0; /*(0,5,5)*/
 
insert into t values(1,1,5); /*(1,1,5)*/
update t set c=5 where id=1; /*(1,5,5)*/
 
update t set d=100 where d=5;/* 所有 d=5 的行，d 改成 100*/
```

```
这个语句序列，不论是拿到备库去执行，还是以后用 binlog 来克隆一个库，这三行的结果，都变成了 (0,5,100)、(1,5,100) 和 (5,5,100)。

也就是说，id=0 和 id=1 这两行，发生了数据不一致。这个问题很严重，是不行的。
```

```
分析知道，这是我们假设“select * from t where d=5 for update 这条语句只给 d=5 这一行，也就是 id=5 的这一行加锁”导致的。

所以我们认为，上面的设定不合理，要改。

那怎么改呢？我们把扫描过程中碰到的行，也都加上写锁，再来看看执行效果。
```

![1677813145728](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677813145728.png)

​																								**图 4 假设扫描到的行都被加上了行锁**

```
由于 session A 把所有的行都加了写锁，所以 session B 在执行第一个 update 语句的时候就被锁住了。需要等到 T6 时刻 session A 提交以后，session B 才能继续执行。

这样对于 id=0 这一行，在数据库里的最终结果还是 (0,5,5)。在 binlog 里面，执行序列是这样的：
```

```sql
insert into t values(1,1,5); /*(1,1,5)*/
update t set c=5 where id=1; /*(1,5,5)*/
 
update t set d=100 where d=5;/* 所有 d=5 的行，d 改成 100*/
 
update t set d=5 where id=0; /*(0,0,5)*/
update t set c=5 where id=0; /*(0,5,5)*/
```

```
可以看到，按照日志顺序执行，id=0 这一行的最终结果也是 (0,5,5)。所以，id=0 这一行的问题解决了。

但同时你也可以看到，id=1 这一行，在数据库里面的结果是 (1,5,5)，而根据 binlog 的执行结果是 (1,5,100)，也就是说幻读的问题还是没有解决。为什么我们已经这么“凶残”地，把所有的记录都上了锁，还是阻止不了 id=1 这一行的插入和更新呢？

原因很简单。在 T3 时刻，我们给所有行加锁的时候，id=1 这一行还不存在，不存在也就加不上锁。

也就是说，即使把所有的记录都加上锁，还是阻止不了新插入的记录，这也是为什么“幻读”会被单独拿出来解决的原因。
```



#### 如何解决幻读？

```
产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。

顾名思义，间隙锁，锁的就是两个值之间的空隙。比如文章开头的表 t，初始化插入了 6 个记录，这就产生了 7 个间隙。如下图所示：
```

![1677813167606](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677813167606.png)

​																							**图 5 表 t 主键索引上的行锁和间隙锁**

```
这样，当你执行 select * from t where d=5 for update 的时候，就不止是给数据库中已有的 6 个记录加上了行锁，还同时加了 7 个间隙锁。这样就确保了无法再插入新的记录。

也就是说这时候，在一行行扫描的过程中，不仅将给行加上了行锁，还给行两边的空隙，也加上了间隙锁。

现在你知道了，数据行是可以加上锁的实体，数据行之间的间隙，也是可以加上锁的实体。但是间隙锁跟我们之前碰到过的锁都不太一样。

比如行锁，分成读锁和写锁。下面的表格就是这两种类型行锁的冲突关系。
```

|      | 读锁 | 写锁 |
| ---- | ---- | ---- |
| 读锁 | 兼容 | 冲突 |
| 写锁 | 冲突 | 冲突 |

```
也就是说，跟行锁有冲突关系的是“另外一个行锁”。

但是间隙锁不一样，跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。
```

![1677813184379](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677813184379.png)

​																												**图 6 间隙锁之间不互锁**

```
这里 session B 并不会被堵住。因为表 t 里并没有 c=7 这个记录，因此 session A 加的是间隙锁 (5,10)。而 session B 也是在这个间隙加的间隙锁。它们有共同的目标，即：保护这个间隙，不允许插入值。但，它们之间是不冲突的。

间隙锁和行锁合称 next-key lock 临键锁，每个 next-key lock 是前开后闭区间。也就是说，我们的表 t 初始化以后，如果用 select * from t for update 要把整个表所有记录锁起来，就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。

因为 +∞是开区间。实现上，InnoDB 给每个索引加了一个不存在的最大值 supremum，这样才符合我们前面说的“都是前开后闭区间”。
```



#### 间隙锁和 next-key lock 解决了幻读但有了别的”困扰“？

![1677813197308](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1677813197308.png)

​																									**图 7 间隙锁导致的死锁**

```
1.session A 执行 select … for update 语句，由于 id=9 这一行并不存在，因此会加上间隙锁 (5,10);
2.session B 执行 select … for update 语句，同样会加上间隙锁 (5,10)，间隙锁之间不会冲突，因此这个语句可以执行成功；
3.session B 试图插入一行 (9,9,9)，被 session A 的间隙锁挡住了，只好进入等待；
4.session A 试图插入一行 (9,9,9)，被 session B 的间隙锁挡住了。

至此，两个 session 进入互相等待状态，形成死锁。当然，InnoDB 的死锁检测马上就发现了这对死锁关系，让 session A 的 insert 语句报错返回了。
```

```
【注】：间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。
```



### 加锁

#### 加锁规则

```
规则只限于截止到现在的最新版本，即 5.x 系列 <=5.7.24，8.0 系列 <=8.0.13。
```

```
加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。
1.原则 1：加锁的基本单位是 next-key lock。next-key lock 是前开后闭区间。
2.原则 2：查找过程中访问到的对象才会加锁。
3.优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。
4.优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。
5.一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。
```

```sql
CREATE TABLE `x` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  `d` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `c` (`c`)
) ENGINE=InnoDB;
 
insert into x values(0,0,0),(5,5,5),
(10,10,10),(15,15,15),(20,20,20),(25,25,25);
```



##### 案例一：等值查询间隙锁

![image-20220117143410227](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117143410227.png)

​																										**图 1 等值查询的间隙锁**

```
根据上面的规则进行判断：
1.id = 7 的行不存在，根据原则1，加锁单位是 next-key lock，sessionA 加锁范围 (5,10]
2.根据优化2，这是一个等值查询，而 id=10 不满足查询条件，next-key lock 退化成间隙锁，因此最终加锁的范围是 (5,10)。

所以 sessionB 要往这个间隙里面插入 id = 8 的数据会被锁住，但是 sessionC 是 ok 的。
```



##### 案例二：非唯一索引等值锁

![image-20220117143832261](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117143832261.png)

​																								**图 2 只加在非唯一索引上的锁**

```
1.根据原则1，加 next-key lock，加锁的范围是 (0,5]。
2.c 是普通索引，因此仅访问 c=5 这一条记录是不能马上停下来的，需要向右遍历，查到 c=10 才放弃。根据原则 2，访问到的都要加锁，因此要给 (5,10] 加 next-key lock。
3.同时这个符合优化 2：等值判断，向右遍历，最后一个值不满足 c=5 这个等值条件，因此退化成间隙锁 (5,10)。
4.根据原则 2 ，只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么 session B 的 update 语句可以执行完成。

所以对于 sessionB 是要给 id = 5 这一行加写锁，不会被阻塞。对于 sessionC 是向 (5,10)这个间隙里面插入 id = 7 的数据，会被锁住。

【注】：lock in share mode 只锁覆盖索引，但是如果是 for update 就不一样了。 执行 for update 时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。
【注】：锁是加在索引上的；同时，如果你要用 lock in share mode 来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。比如，将 session A 的查询语句改成 select d from t where c=5 lock in share mode。你可以自己验证一下效果。
```



##### 案例三：主键索引范围锁

```sql
-- 在逻辑上，这两条查语句肯定是等价的，但是它们的加锁规则不太一样
select * from t where id=10 for update;
select * from t where id>=10 and id<11 for update;
```

![image-20220117145159397](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117145159397.png)

​																									**图 3 主键索引上范围查询的锁**

```
1.对于 sessionA，找到第一个 id=10 的行，因此是 next-key lock (5,10]。根据优化 1，主键 id 上的等值条件，退化成行锁，只加了 id=10 这一行的行锁。
2.范围查找就往后继续找，找到 id=15 这一行停下来，因此需要加 next-key lock (10,15]。

所以，session A 这时候锁的范围就是主键索引上，行锁 id=10 和 next-key lock(10,15]。
对于sessionB 插入 id = 8 的数据不会被阻塞，但是插入 id = 13的行会被 next-key lock (10,15] 锁住，对于 sessionC 一样。

【注】：首次 session A 定位查找 id=10 的行的时候，是当做等值查询来判断的，而向右扫描到 id=15 的时候，用的是范围查询判断。
```



##### 案例四：非唯一索引范围锁

![image-20220117145637075](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117145637075.png)

​																									**图 4 非唯一索引范围锁**

```
1.对于 sessionA 找到第一个 id = 10的行，加 next-key lock (5,10]。索引 c 是普通索引，不存在优化规则，临键锁 不会退化为行锁。
2.范围查询向后查找，找到 id = 15的行，加 next-key lock (10,15]，不存在等值查询则 临键锁 不会退化为间隙锁。

对于sessionB 和 sessionC 来说分别是被 next-key lock (5,10] 锁住，next-key lock (10,15] 锁住。
```



##### 案例五：唯一索引范围锁 bug

![image-20220117150137011](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117150137011.png)

```
session A 是一个范围查询，按照原则 1 的话，应该是索引 id 上只加 (10,15] 这个 next-key lock，并且因为 id 是唯一键，所以循环判断到 id=15 这一行就应该停止了。

但是实现上，InnoDB 会往前扫描到第一个不满足条件的行为止，就是 id=20。而且由于这是个范围扫描，因此索引 id 上的 (15,20] 这个 next-key lock 也会被锁上。

所以 session B 要更新 id=20 这一行，是会被锁住的。同样地，session C 要插入 id=16 的一行，也会被锁住。
照理说，这里锁住 id=20 这一行的行为，其实是没有必要的。因为扫描到 id=15，就可以确定不用往后再找了。但实现上还是这么做了，因此可能是个 bug。
```



##### 案例六：非唯一索引上存在"等值"的例子

```sql
insert into t values(30,10,30);
```

```
新插入的这一行 c=10，也就是说现在表里有两个 c=10 的行。那么，这时候索引 c 上的间隙是什么状态了呢？你要知道，由于非唯一索引上包含主键的值，所以是不可能存在“相同”的两行的。
```

![image-20220117150459840](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117150459840.png)

​																										**图 6 非唯一索引等值的例子**

```
可以看到，虽然有两个 c=10，但是它们的主键值 id 是不同的（分别是 10 和 30），因此这两个 c=10 的记录之间，也是有间隙的。

图中我画出了索引 c 上的主键 id。为了跟间隙锁的开区间形式进行区别，我用 (c=10,id=30) 这样的形式，来表示索引上的一行。
```



![image-20220117150537902](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117150537902.png)

​																												**图 7 delete 示例**

```
这时，session A 在遍历的时候，先访问第一个 c=10 的记录。同样地，根据原则 1，这里加的是 (c=5,id=5) 到 (c=10,id=10) 这个 next-key lock (5,10]。

然后，session A 向右查找，直到碰到 (c=15,id=15) 这一行，循环才结束。根据优化 2，这是一个等值查询，向右查找到了不满足条件的行，所以会退化成 (c=10,id=10) 到 (c=15,id=15) 的间隙锁 (10,15)。

也就是说，这个 delete 语句在索引 c 上的加锁范围，就是下图中蓝色区域覆盖的部分。
```

![image-20220117150720295](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117150720295.png)

​																									**图 8 delete 加锁效果示例**

```
这个蓝色区域左右两边都是虚线，表示开区间，即 (c=5,id=5) 和 (c=15,id=15) 这两行上都没有锁。
```



##### 案例七：limit 语句加锁

![image-20220117150756129](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117150756129.png)

​																										**图 9 limit 语句加锁**

```
session A 的 delete 语句加了 limit 2。你知道表 t 里 c=10 的记录其实只有两条，因此加不加 limit 2，删除的效果都是一样的，但是加锁的效果却不同。可以看到，session B 的 insert 语句执行通过了，跟案例六的结果不同。

这是因为，案例七里的 delete 语句明确加了 limit 2 的限制，因此在遍历到 (c=10, id=30) 这一行之后，满足条件的语句已经有两条，循环就结束了。

因此，索引 c 上的加锁范围就变成了从（c=5,id=5) 到（c=10,id=30) 这个前开后闭区间，如下图所示：
```

![image-20220117151052225](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117151052225.png)

​																								**图 10 带 limit 2 的加锁效果**

```
可以看到，(c=10,id=30）之后的这个间隙并没有在加锁范围里，因此 insert 语句插入 c=12 是可以执行成功的。

【注】：在删除数据的时候尽量加 limit。这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。
```



##### 案例八：一个死锁的例子

```
next-key lock 实际上是间隙锁和行锁加起来的结果。
```

![image-20220117151242496](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117151242496.png)

```
1.根据原则1，查找到 c = 10 这一行加 next-key lock (5,10]。
2.根据原则2，找到 c = 10 这一行，由于等值查询继续向后遍历找到 c = 15的行，不满足条件，根据优化2，临键锁 (10,15] 退化为 间隙锁 加锁范围：(10,15)。
3.对于 sessionB 要向 c = 10 加 next-key lock (5,10]，进入等待。
4.对于 sessionC 要插入 （8,8,8）这一行，被 sessionB 的间隙锁锁住，出现了死锁，则 innodb 让 sessionB 回滚。

session B 的 next-key lock 不是还没申请成功吗？
【答】：是这样的，session B 的“加 next-key lock(5,10] ”操作，实际上分成了两步，先是加 (5,10) 的间隙锁，加锁成功；然后加 c=10 的行锁，这时候才被锁住的。
```



### 性能

### 短连接

```
正常的短连接模式就是连接到数据库后，执行很少的 SQL 语句就断开，下次需要的时候再重连。如果使用的是短连接，在业务高峰期的时候，就可能出现连接数突然暴涨的情况。
MySQL 建立连接的过程，成本是很高的。除了正常的网络连接三次握手外，还需要做登录权限判断和获得这个连接的数据读写权限。

在数据库压力比较小的时候，这些额外的成本并不明显。

但是，短连接模型存在一个风险，就是一旦数据库处理得慢一些，连接数就会暴涨。max_connections 参数，用来控制一个 MySQL 实例同时存在的连接数的上限，超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。对于被拒绝连接的请求来说，从业务角度看就是数据库不可用。

在机器负载比较高的时候，处理现有请求的时间变长，每个连接保持的时间也更长。这时，再有新建连接的话，就可能会超过 max_connections 的限制。

碰到这种情况时，一个比较自然的想法，就是调高 max_connections 的值。但这样做是有风险的。因为设计 max_connections 这个参数的目的是想保护 MySQL，如果我们把它改得太大，让更多的连接都可以进来，那么系统的负载可能会进一步加大，大量的资源耗费在权限验证等逻辑上，结果可能是适得其反，已经连接的线程拿不到 CPU 资源去执行业务的 SQL 请求。
```



#### 第一种方法：先处理掉那些占着连接但是不工作的线程。

#### **第二种方法：减少连接过程的消耗。**



### 慢查询性能问题

#### 索引没有设计好

```
这种场景一般就是通过紧急创建索引来解决。MySQL 5.6 版本以后，创建索引都支持 Online DDL 了，对于那种高峰期数据库已经被这个语句打挂了的情况，最高效的做法就是直接执行 alter table 语句。

比较理想的是能够在备库先执行。假设你现在的服务是一主一备，主库 A、备库 B，这个方案的大致流程是这样的：

1.在备库 B 上执行 set sql_log_bin=off，也就是不写 binlog，然后执行 alter table 语句加上索引；
2.执行主备切换；
3.这时候主库是 B，备库是 A。在 A 上执行 set sql_log_bin=off，然后执行 alter table 语句加上索引。

这是一个“古老”的 DDL 方案。平时在做变更的时候，你应该考虑类似 gh-ost 这样的方案，更加稳妥。但是在需要紧急处理时，上面这个方案的效率是最高的。
```



#### 语句没写好

```
可以通过改写 SQL 语句来处理。MySQL 5.7 提供了 query_rewrite 功能，可以把输入的一种语句改写成另外一种模式。

比如，语句被错误地写成了 select * from t where id + 1 = 10000，你可以通过下面的方式，增加一个语句改写规则。
```

```sql
insert into query_rewrite.rewrite_rules(pattern, replacement, pattern_database) values ("select * from t where id + 1 = ?", "select * from t where id = ? - 1", "db1");
 
call query_rewrite.flush_rewrite_rules();
```

```
这里，call query_rewrite.flush_rewrite_rules() 这个存储过程，是让插入的新规则生效，也就是我们说的“查询重写”。你可以用下图中的方法来确认改写规则是否生效。
```







#### MySQL 选错了索引

```
这时候，应急方案就是给这个语句加上 force index。

同样地，使用查询重写功能，给原来的语句加上 force index，也可以解决这个问题。

上面我和你讨论的由慢查询导致性能问题的三种可能情况，实际上出现最多的是前两种，即：索引没设计好和语句没写好。而这两种情况，恰恰是完全可以避免的。比如，通过下面这个过程，我们就可以预先发现问题。

上线前，在测试环境，把慢查询日志（slow log）打开，并且把 long_query_time 设置成 0，确保每个语句都会被记录入慢查询日志；

在测试表里插入模拟线上的数据，做一遍回归测试；

观察慢查询日志里每类语句的输出，特别留意 Rows_examined 字段是否与预期一致。（我们在前面文章中已经多次用到过 Rows_examined 方法了，相信你已经动手尝试过了。如果还有不明白的，欢迎给我留言，我们一起讨论）。

不要吝啬这段花在上线前的“额外”时间，因为这会帮你省下很多故障复盘的时间。

如果新增的 SQL 语句不多，手动跑一下就可以。而如果是新项目的话，或者是修改了原有项目的 表结构设计，全量回归测试都是必要的。这时候，你需要工具帮你检查所有的 SQL 语句的返回结果。比如，你可以使用开源工具 pt-query-digest(https://www.percona.com/doc/percona-toolkit/3.0/pt-query-digest.html)。
```



#### QPS 突增问题

```
有时候由于业务突然出现高峰，或者应用程序 bug，导致某个语句的 QPS 突然暴涨，也可能导致 MySQL 压力过大，影响服务。

我之前碰到过一类情况，是由一个新功能的 bug 导致的。当然，最理想的情况是让业务把这个功能下掉，服务自然就会恢复。

而下掉一个功能，如果从数据库端处理的话，对应于不同的背景，有不同的方法可用。我这里再和你展开说明一下。
1.一种是由全新业务的 bug 导致的。假设你的 DB 运维是比较规范的，也就是说白名单是一个个加的。这种情况下，如果你能够确定业务方会下掉这个功能，只是时间上没那么快，那么就可以从数据库端直接把白名单去掉。
2.如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开现有连接。这样，这个新功能的连接不成功，由它引发的 QPS 就会变成 0。
3.如果这个新增的功能跟主体功能是部署在一起的，那么我们只能通过处理语句来限制。这时，我们可以使用上面提到的查询重写功能，把压力最大的 SQL 语句直接重写成"select 1"返回。

当然，这个操作的风险很高，需要你特别细致。它可能存在两个副作用：
1.如果别的功能里面也用到了这个 SQL 语句模板，会有误伤；
2.很多业务并不是靠这一个语句就能完成逻辑的，所以如果单独把这一个语句以 select 1 的结果返回的话，可能会导致后面的业务逻辑一起失败。

所以，方案 3 是用于止血的，跟前面提到的去掉权限验证一样，应该是你所有选项里优先级最低的一个方案。
同时你会发现，其实方案 1 和 2 都要依赖于规范的运维体系：虚拟化、白名单机制、业务账号分离。由此可见，更多的准备，往往意味着更稳定的系统。
```



## MySQL是怎么保证数据不丢的？

### binlog 的写入机制

```
binlog 的写入逻辑比较简单：事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。

一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了 binlog cache 的保存问题。

系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。

事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。状态如图 1 所示。
```

![image-20230303111729606](C:\Users\renyunhui\AppData\Roaming\Typora\typora-user-images\image-20230303111729606.png)

​    																							**图 1 binlog 写盘状态**

```
可以看到，每个线程有自己 binlog cache，但是共用同一份 binlog 文件。
1.图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。
2.图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。

write 和 fsync 的时机，是由参数 sync_binlog 控制的：
1.sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync；
2.sync_binlog=1 的时候，表示每次提交事务都会执行 fsync；
3.sync_binlog=N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。

因此，在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。

但是，将 sync_binlog 设置为 N，对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。
```



### redo log 的写入机制

```
redo log buffer 里面的内容，是不是每次生成后都要直接持久化到磁盘呢？

答案是，不需要。

如果事务执行期间 MySQL 发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。

那么，另外一个问题是，事务还没提交的时候，redo log buffer 中的部分日志有没有可能被持久化到磁盘呢？

答案是，确实会有。

这个问题，要从 redo log 可能存在的三种状态说起。这三种状态，对应的就是图 2 中的三个颜色块。
```

![image-20230303111742219](C:\Users\renyunhui\AppData\Roaming\Typora\typora-user-images\image-20230303111742219.png)

​																							**图 2 MySQL redo log 存储状态**

```
这三种状态分别是：
1.存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分；
2.写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面，也就是图中的黄色部分；
3.持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分。
```

```
日志写到 redo log buffer 是很快的，wirte 到 page cache 也差不多，但是持久化到磁盘的速度就慢多了。

为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值：
1.设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ;
2.设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘；
3.设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。

InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。

注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。
```

```
实际上，除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。
1.一种是，redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。
2.另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。

这里需要说明的是，我们介绍两阶段提交的时候说过，时序上 redo log 先 prepare， 再写 binlog，最后再把 redo log commit。

如果把 innodb_flush_log_at_trx_commit 设置成 1，那么 redo log 在 prepare 阶段就要持久化一次，因为有一个崩溃恢复逻辑是要依赖于 prepare 的 redo log，再加上 binlog 来恢复的。
```

```
每秒一次后台轮询刷盘，再加上崩溃恢复这个逻辑，InnoDB 就认为 redo log 在 commit 的时候就不需要 fsync 了，只会 write 到文件系统的 page cache 中就够了。

通常我们说 MySQL 的“双 1”配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。

这时候，你可能有一个疑问，这意味着我从 MySQL 看到的 TPS 是每秒两万的话，每秒就会写四万次磁盘。但是，我用工具测试出来，磁盘能力也就两万左右，怎么能实现两万的 TPS？

解释这个问题，就要用到组提交（group commit）机制了。

这里，我需要先和你介绍日志逻辑序列号（log sequence number，LSN）的概念。LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。

LSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log。

如图 3 所示，是三个并发事务 (trx1, trx2, trx3) 在 prepare 阶段，都写完 redo log buffer，持久化到磁盘的过程，对应的 LSN 分别是 50、120 和 160。
```



​																								**图 3 redo log 组提交**

```
从图中可以看到，
1.trx1 是第一个到达的，会被选为这组的 leader；
2.等 trx1 要开始写盘的时候，这个组里面已经有了三个事务，这时候 LSN 也变成了 160；
3.trx1 去写盘的时候，带的就是 LSN=160，因此等 trx1 返回时，所有 LSN 小于等于 160 的 redo log，都已经被持久化到磁盘；
4.这时候 trx2 和 trx3 就可以直接返回了。
```

```
所以，一次组提交里面，组员越多，节约磁盘 IOPS 的效果越好。但如果只有单线程压测，那就只能老老实实地一个事务对应一次持久化操作了。

在并发更新场景下，第一个事务写完 redo log buffer 以后，接下来这个 fsync 越晚调用，组员可能越多，节约 IOPS 的效果就越好。

为了让一次 fsync 带的组员更多，MySQL 有一个很有趣的优化：拖时间。
```

![image-20230303111850601](C:\Users\renyunhui\AppData\Roaming\Typora\typora-user-images\image-20230303111850601.png)

​																														**图 4 两阶段提交**

```
图中，我把“写 binlog”当成一个动作。但实际上，写 binlog 是分成两步的：
1.先把 binlog 从 binlog cache 中写到磁盘上的 binlog 文件；
2.调用 fsync 持久化。
```

```
MySQL 为了让组提交的效果更好，把 redo log 做 fsync 的时间拖到了步骤 1 之后。也就是说，上面的图变成了这样：
```

![image-20230303111859404](C:\Users\renyunhui\AppData\Roaming\Typora\typora-user-images\image-20230303111859404.png)

​																												**图 5 两阶段提交细化**

```
这么一来，binlog 也可以组提交了。在执行图 5 中第 4 步把 binlog fsync 到磁盘时，如果有多个事务的 binlog 已经写完了，也是一起持久化的，这样也可以减少 IOPS 的消耗。

不过通常情况下第 3 步执行得会很快，所以 binlog 的 write 和 fsync 间的间隔时间短，导致能集合到一起持久化的 binlog 比较少，因此 binlog 的组提交的效果通常不如 redo log 的效果那么好。

如果你想提升 binlog 组提交的效果，可以通过设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 来实现。
1.binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync;
2.binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync。
这两个条件是或的关系，也就是说只要有一个满足条件就会调用 fsync。
所以，当 binlog_group_commit_sync_delay 设置为 0 的时候，binlog_group_commit_sync_no_delay_count 也无效了。
```

```
WAL 机制主要得益于两个方面：
1.redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快；
2.组提交机制，可以大幅度降低磁盘的 IOPS 消耗。
```

```
如果你的 MySQL 现在出现了性能瓶颈，而且瓶颈在 IO 上，可以通过哪些方法来提升性能呢？

针对这个问题，可以考虑以下三种方法：
1.设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但没有丢失数据的风险。

2.将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。这样做的风险是，主机掉电时会丢 binlog 日志。

3.将 innodb_flush_log_at_trx_commit 设置为 2。这样做的风险是，主机掉电的时候会丢数据。

不建议你把 innodb_flush_log_at_trx_commit 设置成 0。因为把这个参数设置成 0，表示 redo log 只保存在内存中，这样的话 MySQL 本身异常重启也会丢数据，风险太大。而 redo log 写到文件系统的 page cache 的速度也是很快的，所以将这个参数设置成 2 跟设置成 0 其实性能差不多，但这样做 MySQL 异常重启时就不会丢数据了，相比之下风险会更小。
```



### 答疑问题

```
问题 1：执行一个 update 语句以后，我再去执行 hexdump 命令直接查看 ibd 文件内容，为什么没有看到数据有改变呢？
回答：这可能是因为 WAL 机制的原因。update 语句执行完成后，InnoDB 只保证写完了 redo log、内存，可能还没来得及将数据写到磁盘。

问题 2：为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的？
回答：MySQL 这么设计的主要原因是，binlog 是不能“被打断的”。一个事务的 binlog 必须连续写，因此要整个事务完成后，再一起写到文件里。
而 redo log 并没有这个要求，中间有生成的日志可以写到 redo log buffer 中。redo log buffer 中的内容还能“搭便车”，其他事务提交的时候可以被一起写到磁盘中。还有就是 bin log 存储是以 statement 或者 row 格式存储的，而 redo log 是以 page 页格式存储的。page 格式，天生就是共有的，而 row 格式，只跟当前事务相关。

问题 3：事务执行期间，还没到提交阶段，如果发生 crash 的话，redo log 肯定丢了，这会不会导致主备不一致呢？
回答：不会。因为这时候 binlog 也还在 binlog cache 里，没发给备库。crash 以后 redo log 和 binlog 都没有了，从业务角度看这个事务也没有提交，所以数据是一致的。

问题 4：如果 binlog 写完盘以后发生 crash，这时候还没给客户端答复就重启了。等客户端再重连进来，发现事务已经提交成功了，这是不是 bug？
回答：不是。
你可以设想一下更极端的情况，整个事务都提交成功了，redo log commit 完成了，备库也收到 binlog 并执行了。但是主库和客户端网络断开了，导致事务成功的包返回不回去，这时候客户端也会收到“网络断开”的异常。这种也只能算是事务成功的，不能认为是 bug。

实际上数据库的 crash-safe 保证的是：
1.如果客户端收到事务成功的消息，事务就一定持久化了；
2.如果客户端收到事务失败（比如主键冲突、回滚等）的消息，事务就一定失败了；
3.如果客户端收到“执行异常”的消息，应用需要重连后通过查询当前状态来继续后续的逻辑。此时数据库只需要保证内部（数据和日志之间，主库和备库之间）一致就可以了。
```



### 思考题

#### 思考题一

```
我们用了事务来确保计数准确。由于事务可以保证中间结果不被别的事务读到，因此修改计数值和插入新记录的顺序是不影响逻辑结果的。但是，从并发系统性能的角度考虑，你觉得在这个事务序列里，应该先插入操作记录，还是应该先更新计数表呢？
答：从并发系统性能的角度考虑，应该先插入操作记录，再更新计数表。
因为更新计数表涉及到行锁的竞争，先插入再更新能最大程度地减少了事务之间的锁等待，提升了并发度。
```



#### 思考题二

```
假设你的表里面已经有了 city_name(city, name) 这个联合索引，然后你要查杭州和苏州两个城市中所有的市民的姓名，并且按名字排序，显示前 100 条记录。如果 SQL 
查询语句是这么写的 ：

select * from t where city in ('杭州'," 苏州 ") order by name limit 100;

这个语句执行的时候会有排序过程吗，为什么？

如果业务端代码由你来开发，需要实现一个在数据库端不需要排序的方案，你会怎么实现呢？

进一步地，如果有分页需求，要显示第 101 页，也就是说语句最后要改成 “limit 10000,100”， 你的实现方法又会是什么呢？

答：1.会有排序的过程，虽然有 (city,name) 联合索引，对于单个 city 内部，name 是递增的。但是由于这条 SQL 语句不是要单独地查一个 city 的值，而是同时查了"杭州"和" 苏州 "两个城市，因此所有满足条件的 name 就不是递增的了。也就是说，这条 SQL 语句需要排序。

2.如何避免排序：
    2.1.执行 select * from t where city=“杭州” order by name limit 100; 这个语句是不需要排序的，客户端用一个长度为 100 的内存数组 A 保存结果。
    2.2.执行 select * from t where city=“苏州” order by name limit 100; 用相同的方法，假设结果被存进了内存数组 B。
    2.3.现在 A 和 B 是两个有序数组，然后你可以用归并排序的思想，得到 name 最小的前 100 值，就是我们需要的结果了。
    
3.语句最后要改成 “limit 10000,100”
	3.1.执行 select * from t where city=" 杭州 " order by name limit 10100; 
	3.2.执行 select * from t where city=" 苏州 " order by name limit 10100。
	3.3.这时候数据量较大，可以同时起两个连接一行行读结果，用归并排序算法拿到这两个结果集里，按顺序取第 10001~10100 的 name 值，就是需要的结果了。
	
这个方案的损失就是从数据库返回给客户端的数据量变大了。
所以，如果数据的单行比较大的话,可以做一下优化：
	3.1.执行 select id,name from t where city=" 杭州 " order by name limit 10100; 
	3.2.执行 select id,name from t where city=" 苏州 " order by name limit 10100。
	3.3.然后，再用归并排序的方法取得按 name 顺序第 10001~10100 的 name、id 的值，然后拿着这 100 个 id 到数据库中去查出所有记录。
```



#### 思考题三

```
上面的随机算法 3 的总扫描行数是 C+(Y1+1)+(Y2+1)+(Y3+1)，实际上它还是可以继续优化，来进一步减少扫描行数的。
问题是，如果你是这个需求的开发人员，你会怎么做，来减少扫描行数呢？说说你的方案，并说明你的方案需要的扫描行数。

答：取 Y1、Y2 和 Y3 里面最大的一个数，记为 M，最小的一个数记为 N，然后执行下面这条 SQL 语句：
select * from t limit N, M-N+1;
再加上取整个表总行数的 C 行，这个方案的扫描行数总共只需要 C+M+1 行。
当然也可以先取回 id 值，在应用中确定了三个 id 值以后，再执行三次 where id=X 的语句也是可以的。
```



#### 思考题四

```sql
-- 在举例加锁读的时候，用的是这个语句，select * from t where id=1 lock in share mode。由于 id 上有索引，所以可以直接定位到 id=1 这一行，因此读锁也是只加在了这一行上。

-- 但如果是下面的 SQL 语句：

begin;
select * from t where d=5 for update;
commit;
```

```
这个语句序列是怎么加锁的呢？加的锁又是什么时候释放呢？
```

```sql
CREATE TABLE `a` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  `d` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
) ENGINE=InnoDB;
 
insert into a values(0,0,0),(5,5,5),
(10,10,10),(15,15,15),(20,20,20),(25,25,25);
```

```sql
begin;
select * from t where d=5 for update;
commit;
```

```
1.在 Read Committed 隔离级别下，语句执行时，因为字段 d 没有索引，会扫描主键索引，会锁上主键索引中的所有记录（X锁）。在语句执行完成后，是只有行锁的。而且语句执行完成后，InnoDB 就会把不满足条件的行行锁去掉。当然了，d = 5 这一行的行锁，还是会等到 commit 的时候才释放的。

2.在 Repeatable Read 隔离级别下，语句执行时会锁上主键索引中的所有记录（X锁），并且会锁上主键索引内的所有 GAP（意向锁 IX锁）；并且在语句执行完成后，不会释放锁，只有在 commit 的时候才会释放。
```



#### 思考题五

![image-20220117140639400](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220117140639400.png)

​																										**图 8 事务进入锁等待状态**

```
这里 session B 和 session C 的 insert 语句都会进入锁等待状态。出现这种情况的原因是什么？

1.由于是 order by c desc，第一个要定位的是索引 c 上“最右边的”c=20 的行，所以会加上间隙锁 (20,25) 和 next-key lock (15,20]。
2.在索引 c 上向左遍历，要扫描到 c=10 才停下来，所以 next-key lock 会加到 (5,10]，这正是阻塞 session B 的 insert 语句的原因。
3.在扫描过程中，c=20、c=15、c=10 这三行都存在值，由于是 select *，不存在覆盖索引，所以会在主键 id 上加三个行锁。

因此，session A 的 select 语句锁的范围就是：
1.索引 c 上 (5, 25)；
2.主键索引上 id=15、20 两个行锁。

对于 sessionB 要插入间隙 (10,15) 被 sessionA 加的 next-key lock (10,15] 锁住。
对于 sessionc 要插入间隙 (5,10) 被 sessionA 加的 next-key lock (5,10] 锁住。
```



## MySQL是怎么保证主备一致的？

### MySQL 主备的基本原理

![image-20220719172833498](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20220719172833498.png)

​																										**图 1 MySQL 主备切换流程**

```
如图 1 所示就是基本的主备切换流程。

在状态 1 中，客户端的读写都直接访问节点 A，而节点 B 是 A 的备库，只是将 A 的更新都同步过来，到本地执行。这样可以保持节点 B 和 A 的数据是相同的。
当需要切换的时候，就切成状态 2。这时候客户端读写访问的都是节点 B，而节点 A 是 B 的备库。

在状态 1 中，虽然节点 B 没有被直接访问，但是建议把节点 B（也就是备库）设置成只读（readonly）模式。这样做，有以下几个考虑：
1.有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作；
2.防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致；
3.可以用 readonly 状态，来判断节点的角色。
```

```
我把备库设置成只读了，还怎么跟主库保持同步更新呢？
这个问题，你不用担心。因为 readonly 设置对超级 (super) 权限用户是无效的，而用于同步更新的线程，就拥有超级权限。
```

```
节点 A 到 B 这条线的内部流程是什么样的。图 2 中画出的就是一个 update 语句在节点 A 执行，然后同步到节点 B 的完整流程图。
```

![1658221196540](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/1658221196540.png)

​																											**图 2 主备流程图**

```
图 2 中，包含了 bin log 和 redo log 的写入机制相关的内容，可以看到：主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写 binlog。

备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的：
1.在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。
2.在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。
3.主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。
4.备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。
5.sql_thread 读取中转日志，解析出日志里的命令，并执行。

这里需要说明，后来由于多线程复制方案的引入，sql_thread 演化成为了多个线程，跟我们今天要介绍的原理没有直接关系，暂且不展开。
分析完了这个长连接的逻辑，我们再来看一个问题：binlog 里面到底是什么内容，为什么备库拿过去可以直接执行。
```



### binlog 的三种格式对比

#### statement

```sql
CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `a` int(11) DEFAULT NULL,
  `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`),
  KEY `a` (`a`),
  KEY `t_modified`(`t_modified`)
) ENGINE=InnoDB;
 
insert into t values(1,1,'2018-11-13');
insert into t values(2,2,'2018-11-12');
insert into t values(3,3,'2018-11-11');
insert into t values(4,4,'2018-11-10');
insert into t values(5,5,'2018-11-09');
```

```sql
-- 如果要在表中删除一行数据的话，我们来看看这个 delete 语句的 binlog 是怎么记录的。
delete from t /*comment*/  where a>=4 and t_modified<='2018-11-10' limit 1;

-- 当 binlog_format=statement 时，binlog 里面记录的就是 SQL 语句的原文。你可以用
show binlog events in 'master.000001';
```

![image-20220121155623646](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220121155623646.png)

​																								**图 3 statement 格式 binlog 示例**

```
看一下图 3 的输出结果。
1.第一行 SET @@SESSION.GTID_NEXT='ANONYMOUS’你可以先忽略，后面文章我们会在介绍主备切换的时候再提到；
2.第二行是一个 BEGIN，跟第四行的 commit 对应，表示中间是一个事务；
3.第三行就是真实执行的语句了。可以看到，在真实执行的 delete 命令之前，还有一个“use ‘test’”命令。这条命令不是我们主动执行的，而是 MySQL 根据当前要操作的表所在的数据库，自行添加的。这样做可以保证日志传到备库去执行的时候，不论当前的工作线程在哪个库里，都能够正确地更新到 test 库的表 t。
use 'test’命令之后的 delete 语句，就是我们输入的 SQL 原文了。可以看到，binlog“忠实”地记录了 SQL 命令，甚至连注释也一并记录了。
4.最后一行是一个 COMMIT。你可以看到里面写着 xid=61。（redo log 和 bin log 进行关联）
```

![image-20220121160218475](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220121160218475.png)

​																									**图 4 delete 执行 warnings**

```
运行这条 delete 命令产生了一个 warning，原因是当前 binlog 设置的是 statement 格式，并且语句中有 limit，所以这个命令可能是 unsafe 的。

这是因为 delete 带 limit，很可能会出现主备数据不一致的情况。比如上面这个例子：
1.如果 delete 语句使用的是索引 a，那么会根据索引 a 找到第一个满足条件的行，也就是说删除的是 a=4 这一行；
2.但如果使用的是索引 t_modified，那么删除的就是 t_modified='2018-11-09’也就是 a=5 这一行。

由于 statement 格式下，记录到 binlog 里的是语句原文，因此可能会出现这样一种情况：在主库执行这条 SQL 语句的时候，用的是索引 a；而在备库执行这条 SQL 语句的时候，却使用了索引 t_modified。因此，MySQL 认为这样写是有风险的。
```



#### row

```
如果我把 binlog 的格式改为 binlog_format=‘row’， 是不是就没有这个问题了呢？
```

![image-20220121160826859](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220121160826859.png)

​																										**图 5 row 格式 binlog 示例**

```
可以看到，与 statement 格式的 binlog 相比，前后的 BEGIN 和 COMMIT 是一样的。但是，row 格式的 binlog 里没有了 SQL 语句的原文，而是替换成了两个 event：Table_map 和 Delete_rows。
1.Table_map event，用于说明接下来要操作的表是 test 库的表 t;
2.Delete_rows event，用于定义删除的行为。
```

```sql
-- 其实，我们通过图 5 是看不到详细信息的，还需要借助 mysqlbinlog 工具，用下面这个命令解析和查看 binlog 中的内容。因为图 5 中的信息显示，这个事务的 binlog 是从 8900 这个位置开始的，所以可以用 start-position 参数来指定从这个位置的日志开始解析。

mysqlbinlog  -vv data/master.000001 --start-position=6705;
```

![image-20220121162052625](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220121162052625.png)

​																					**图 6 row 格式 binlog 示例的详细信息**

```
从这个图中，我们可以看到以下几个信息：
1.server id 1，表示这个事务是在 server_id=1 的这个库上执行的。
2.每个 event 都有 CRC32 的值，这是因为我把参数 binlog_checksum 设置成了 CRC32。
3.Table_map event 跟在图 5 中看到的相同，显示了接下来要打开的表，map 到数字 226。现在我们这条 SQL 语句只操作了一张表，如果要操作多张表呢？每个表都有一个对应的 Table_map event、都会 map 到一个单独的数字，用于区分对不同表的操作。
4.我们在 mysqlbinlog 的命令中，使用了 -vv 参数是为了把内容都解析出来，所以从结果里面可以看到各个字段的值（比如，@1=4、 @2=4 这些值）。
5.binlog_row_image 的默认配置是 FULL，因此 Delete_event 里面，包含了删掉的行的所有字段的值。如果把 binlog_row_image 设置为 MINIMAL，则只会记录必要的信息，在这个例子里，就是只会记录 id=4 这个信息。
6.最后的 Xid event，用于表示事务被正确地提交了。

当 binlog_format 使用 row 格式的时候，binlog 里面记录了真实删除行的主键 id，这样 binlog 传到备库去的时候，就肯定会删除 id=4 的行，不会有主备删除不同行的问题。
```



#### mixed

```
基于上面的信息，我们来讨论一个问题：为什么会有 mixed 这种 binlog 格式的存在场景？推论过程是这样的：
1.因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。
2.但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。
3.所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。

也就是说，mixed 格式可以利用 statment 格式的优点，同时又避免了数据不一致的风险。
```

```
比如我们这个例子，设置为 mixed 后，就会记录为 row 格式；而如果执行的语句去掉 limit 1，就会记录为 statement 格式。

当然我要说的是，现在越来越多的场景要求把 MySQL 的 binlog 格式设置成 row。这么做的理由有很多，我来给你举一个可以直接看出来的好处：恢复数据。

接下来，我们就分别从 delete、insert 和 update 这三种 SQL 语句的角度，来看看数据恢复的问题。

通过图 6 你可以看出来，即使我执行的是 delete 语句，row 格式的 binlog 也会把被删掉的行的整行信息保存起来。所以，如果你在执行完一条 delete 语句以后，发现删错数据了，可以直接把 binlog 中记录的 delete 语句转成 insert，把被错删的数据插入回去就可以恢复了。

如果你是执行错了 insert 语句呢？那就更直接了。row 格式下，insert 语句的 binlog 里会记录所有的字段信息，这些信息可以用来精确定位刚刚被插入的那一行。这时，你直接把 insert 语句转成 delete 语句，删除掉这被误插入的一行数据就可以了。

如果执行的是 update 语句的话，binlog 里面会记录修改前整行的数据和修改后的整行数据。所以，如果你误执行了 update 语句的话，只需要把这个 event 前后的两行信息对调一下，再去数据库里面执行，就能恢复这个更新操作了。

其实，由 delete、insert 或者 update 语句导致的数据操作错误，需要恢复到操作之前状态的情况，也时有发生。MariaDB 的Flashback工具就是基于上面介绍的原理来回滚数据的。
```

```sql
-- 虽然 mixed 格式的 binlog 现在已经用得不多了，但这里我还是要再借用一下 mixed 格式来说明一个问题，来看一下这条 SQL 语句：
insert into t values(10,10, now());
```

```
如果我们把 binlog 格式设置为 mixed，你觉得 MySQL 会把它记录为 row 格式还是 statement 格式呢？
```

![image-20220121162940511](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220121162940511.png)

​																									**图 7 mixed 格式和 now()**

```
可以看到，MySQL 用的居然是 statement 格式。你一定会奇怪，如果这个 binlog 过了 1 分钟才传给备库的话，那主备的数据不就不一致了吗？
```

![image-20220121163243203](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220121163243203.png)

​																										**图 8 TIMESTAMP 命令**

```
从图中的结果可以看到，原来 binlog 在记录 event 的时候，多记了一条命令：SET TIMESTAMP=1546103491。它用 SET TIMESTAMP 命令约定了接下来的 now() 函数的返回时间。

因此，不论这个 binlog 是 1 分钟之后被备库执行，还是 3 天后用来恢复这个库的备份，这个 insert 语句插入的行，值都是固定的。也就是说，通过这条 SET TIMESTAMP 命令，MySQL 就确保了主备数据的一致性。

我之前看过有人在重放 binlog 数据的时候，是这么做的：用 mysqlbinlog 解析出日志，然后把里面的 statement 语句直接拷贝出来执行。

你现在知道了，这个方法是有风险的。因为有些语句的执行结果是依赖于上下文命令的，直接执行的结果很可能是错误的。

所以，用 binlog 来恢复数据的标准做法是，用 mysqlbinlog 工具解析出来，然后把解析结果整个发给 MySQL 执行。类似下面的命令：
```

```sql
mysqlbinlog master.000001  --start-position=2738 --stop-position=2973 | mysql -h127.0.0.1 -P13000 -u$user -p$pwd;

-- 这个命令的意思是，将 master.000001 文件里面从第 2738 字节到第 2973 字节中间这段内容解析出来，放到 MySQL 去执行。
```



### 循环复制问题

```
通过上面对 MySQL 中 binlog 基本内容的理解，你现在可以知道，binlog 的特性确保了在备库执行相同的 binlog，可以得到与主库相同的状态。

因此，我们可以认为正常情况下主备的数据是一致的。也就是说，图 1 中 A、B 两个节点的内容是一致的。其实，图 1 中我画的是 M-S 结构，但实际生产上使用比较多的是双 M 结构，也就是图 9 所示的主备切换流程。
```

![image-20220121163323349](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220121163323349.png)

​																					**图 9 MySQL 主备切换流程 -- 双 M 结构**

```
对比图 9 和图 1，你可以发现，双 M 结构和 M-S 结构，其实区别只是多了一条线，即：节点 A 和 B 之间总是互为主备关系。这样在切换的时候就不用再修改主备关系。
但是，双 M 结构还有一个问题需要解决。
业务逻辑在节点 A 上更新了一条语句，然后再把生成的 binlog 发给节点 B，节点 B 执行完这条更新语句后也会生成 binlog。（我建议你把参数 log_slave_updates 设置为 on，表示备库执行 relay log 后生成 binlog）。

那么，如果节点 A 同时是节点 B 的备库，相当于又把节点 B 新生成的 binlog 拿过来执行了一次，然后节点 A 和 B 间，会不断地循环执行这个更新语句，也就是循环复制了。这个要怎么解决呢？
从上面的图 6 中可以看到，MySQL 在 binlog 中记录了这个命令第一次执行时所在实例的 server id。因此，我们可以用下面的逻辑，来解决两个节点间的循环复制的问题：
1.规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系；
2.一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog；
3.每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。

按照这个逻辑，如果我们设置了双 M 结构，日志的执行流就会变成这样：
1.从节点 A 更新的事务，binlog 里面记的都是 A 的 server id；
2.传到节点 B 执行一次以后，节点 B 生成的 binlog 的 server id 也是 A 的 server id；
3.再传回给节点 A，A 判断到这个 server id 与自己的相同，就不会再处理这个日志。所以，死循环在这里就断掉了。
```



```
1.双M的架构情况，主从复制的判断依据：
一开始创建主备关系的时候， 是由备库指定的。比如基于位点的主备关系，备库说“我要从binlog文件A的位置P”开始同步， 主库就从这个指定的位置开始往后发。
而主备复制关系搭建完成以后，是主库来决定“要发数据给备库”的。所以主库有生成新的日志，就会发给备库。

2.主库 A 从本地读取 binlog，发给从库 B；
	对于A的线程来说，就是“读文件”，
	2.1. 如果这个文件现在还在 page cache中，那就最好了，直接读走；
	2.2. 如果不在page cache里，就只好去磁盘读
这个行为是文件系统控制的，MySQL只是执行“读文件”这个操作


```



## MySQL是怎么保证高可用的？

```
在一个主备关系中，每个备库接收主库的 binlog 并执行。

正常情况下，只要主库执行更新生成的所有 binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。
但是，MySQL 要提供高可用能力，只有最终一致性是不够的。为什么这么说呢？

如下是：双 M 结构的主备切换流程图
```

![image-20220121163323349](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220121163323349.png)

​																								**图 1 MySQL 主备切换流程 -- 双 M 结构**

### 主备延迟

```
主备切换可能是一个主动运维动作，比如软件升级、主库所在机器按计划下线等，也可能是被动操作，比如主库所在机器掉电。

主动切换的场景。
在介绍主动切换流程的详细步骤之前，我要先跟你说明一个概念，即“同步延迟”。与数据同步有关的时间点主要包括以下三个：
1.主库 A 执行完成一个事务，写入 binlog，我们把这个时刻记为 T1;
2.之后传给备库 B，我们把备库 B 接收完这个 binlog 的时刻记为 T2;
3.备库 B 执行完成这个事务，我们把这个时刻记为 T3。

所谓主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是 T3-T1。
你可以在备库上执行 show slave status 命令，它的返回结果里面会显示 seconds_behind_master，用于表示当前备库延迟了多少秒。

seconds_behind_master 的计算方法是这样的：
1.每个事务的 binlog 里面都有一个时间字段，用于记录主库上写入的时间；
2.备库取出当前正在执行的事务的时间字段的值，计算它与当前系统时间的差值，得到 seconds_behind_master。

可以看到，其实 seconds_behind_master 这个参数计算的就是 T3-T1。所以，我们可以用 seconds_behind_master 来作为主备延迟的值，这个值的时间精度是秒。

如果主备库机器的系统时间设置不一致，会不会导致主备延迟的值不准？
其实不会的。因为，备库连接到主库的时候，会通过执行 SELECT UNIX_TIMESTAMP() 函数来获得当前主库的系统时间。如果这时候发现主库的系统时间与自己不一致，备库在执行 seconds_behind_master 计算的时候会自动扣掉这个差值。

需要说明的是，在网络正常的时候，日志从主库传给备库所需的时间是很短的，即 T2-T1 的值是非常小的。也就是说，网络正常情况下，主备延迟的主要来源是备库接收完 binlog 和执行完这个事务之间的时间差。

所以说，主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产 binlog 的速度要慢。
```



### 主备延迟的来源

#### 有些部署条件下，备库所在机器的性能要比主库所在的机器性能差

```
一般情况下，有人这么部署时的想法是，反正备库没有请求，所以可以用差一点儿的机器。或者，他们会把 20 个主库放在 4 台机器上，而把备库集中在一台机器上。

其实我们都知道，更新请求对 IOPS 的压力，在主库和备库上是无差别的。所以，做这种部署时，一般都会将备库设置为“非双 1”的模式。

但实际上，更新过程中也会触发大量的读操作。所以，当备库主机上的多个备库都在争抢资源的时候，就可能会导致主备延迟了。

当然，这种部署现在比较少了。因为主备可能发生切换，备库随时可能变成主库，所以主备库选用相同规格的机器，并且做对称部署，是现在比较常见的情况。

做了对称部署以后，还可能会有延迟?（原因如：备库的压力过大）
```



#### 备库的压力大

```
主库既然提供了写能力，那么备库可以提供一些读能力。或者一些运营后台需要的分析语句，不能影响正常业务，所以只能在备库上跑。

我真就见过不少这样的情况。由于主库直接影响业务，大家使用起来会比较克制，反而忽视了备库的压力控制。结果就是，备库上的查询耗费了大量的 CPU 资源，影响了同步速度，造成主备延迟。

这种情况，我们一般可以这么处理：
1.一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力。
2.通过 binlog 输出到外部系统，比如 Hadoop 这类系统，让外部系统提供统计类查询的能力。

其中，一主多从的方式大都会被采用。因为作为数据库系统，还必须保证有定期全量备份的能力。而从库，就很适合用来做备份。

采用了一主多从，保证备库的压力不会超过主库，还有什么情况可能导致主备延迟吗？（原因如：大事务）
```



#### 大事务

```
大事务这种情况很好理解。因为主库上必须等事务执行完成才会写入 binlog，再传给备库。所以，如果一个主库上的语句执行 10 分钟，那这个事务很可能就会导致从库延迟 10 分钟。

不知道你所在公司的 DBA 有没有跟你这么说过：不要一次性地用 delete 语句删除太多数据。其实，这就是一个典型的大事务场景。

比如，一些归档类的数据，平时没有注意删除历史数据，等到空间快满了，业务开发人员要一次性地删掉大量历史数据。同时，又因为要避免在高峰期操作会影响业务（至少有这个意识还是很不错的），所以会在晚上执行这些大量数据的删除操作。

结果，负责的 DBA 同学半夜就会收到延迟报警。然后，DBA 团队就要求你后续再删除数据的时候，要控制每个事务删除的数据量，分成多次删除。
```



#### 备库的并行复制能力





### 可靠性优先策略

```
在图 1 的双 M 结构下，从状态 1 到状态 2 切换的详细过程是这样的：

1.判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步；
2.把主库 A 改成只读状态，即把 readonly 设置为 true；
3.判断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止；
4.把备库 B 改成可读写状态，也就是把 readonly 设置为 false；
5.把业务请求切到备库 B。

这个切换流程，一般是由专门的 HA 系统来完成的，我们暂时称之为可靠性优先流程。
```

![image-20220125135409272](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220125135409272.png)

​										       											**图 2 MySQL 可靠性优先主备切换流程**

```
备注：图中的 SBM，是 seconds_behind_master 参数的简写。

可以看到，这个切换流程中是有不可用时间的。因为在步骤 2 之后，主库 A 和备库 B 都处于 readonly 状态，也就是说这时系统处于不可写状态，直到步骤 5 完成后才能恢复。

在这个不可用状态中，比较耗费时间的是步骤 3，可能需要耗费好几秒的时间。这也是为什么需要在步骤 1 先做判断，确保 seconds_behind_master 的值足够小。

试想如果一开始主备延迟就长达 30 分钟，而不先做判断直接切换的话，系统的不可用时间就会长达 30 分钟，这种情况一般业务都是不可接受的。

当然，系统的不可用时间，是由这个数据可靠性优先的策略决定的。你也可以选择可用性优先的策略，来把这个不可用时间几乎降为 0。
```



### 可用性优先策略

```
如果我强行把步骤 4、5 调整到最开始执行，也就是说不等主备数据同步，直接把连接切到备库 B，并且让备库 B 可以读写，那么系统几乎就没有不可用时间了。

我们把这个切换流程，暂时称作可用性优先流程。这个切换流程的代价，就是可能出现数据不一致的情况。
```

```sql
-- 接下来，我就和你分享一个可用性优先流程产生数据不一致的例子。假设有一个表 t：

CREATE TABLE `r` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `c` int(11) unsigned DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
 
insert into r(c) values(1),(2),(3);

-- 这个表定义了一个自增主键 id，初始化数据后，主库和备库上都是 3 行数据。接下来，业务人员要继续在表 t 上执行两条插入语句的命令，依次是：

insert into t(c) values(4);
insert into t(c) values(5);
```

```
假设，现在主库上其他的数据表有大量的更新，导致主备延迟达到 5 秒。在插入一条 c=4 的语句后，发起了主备切换。
```

![image-20220125135908236](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220125135908236.png)

​																						**图 3 可用性优先策略，且 binlog_format=mixed**

```
1.步骤 2 中，主库 A 执行完 insert 语句，插入了一行数据（4,4），之后开始进行主备切换。
2.步骤 3 中，由于主备之间有 5 秒的延迟，所以备库 B 还没来得及应用“插入 c=4”这个中转日志，就开始接收客户端“插入 c=5”的命令。
3.步骤 4 中，备库 B 插入了一行数据（4,5），并且把这个 binlog 发给主库 A。
4.步骤 5 中，备库 B 执行“插入 c=4”这个中转日志，插入了一行数据（5,4）。而直接在备库 B 执行的“插入 c=5”这个语句，传到主库 A，就插入了一行新数据（5,5）。

最后的结果就是，主库 A 和备库 B 上出现了两行不一致的数据。可以看到，这个数据不一致，是由可用性优先流程导致的。
```

```
那么，如果我还是用可用性优先策略，但设置 binlog_format=row，情况又会怎样呢？

因为 row 格式在记录 binlog 的时候，会记录新插入的行的所有字段值，所以最后只会有一行不一致。而且，两边的主备同步的应用线程会报错 duplicate key error 并停止。也就是说，这种情况下，备库 B 的 (5,4) 和主库 A 的 (5,5) 这两行数据，都不会被对方执行。
```

![image-20220125140242685](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220125140242685.png)

​																				**图 4 可用性优先策略，且 binlog_format=row**

```
从上面的分析中，你可以看到一些结论：
1.使用 row 格式的 binlog 时，数据不一致的问题更容易被发现。而使用 mixed 或者 statement 格式的 binlog 时，数据很可能悄悄地就不一致了。如果你过了很久才发现数据不一致的问题，很可能这时的数据不一致已经不可查，或者连带造成了更多的数据逻辑不一致。
2.主备切换的可用性优先策略会导致数据不一致。因此，大多数情况下，我都建议你使用可靠性优先策略。毕竟对数据服务来说的话，数据的可靠性一般还是要优于可用性的。
```

```
但事无绝对，有没有哪种情况数据的可用性优先级更高呢？

答案是，有的。

我曾经碰到过这样的一个场景：
有一个库的作用是记录操作日志。这时候，如果数据不一致可以通过 binlog 来修补，而这个短暂的不一致也不会引发业务问题。
同时，业务系统依赖于这个日志写入逻辑，如果这个库不可写，会导致线上的业务操作无法执行。
这时候，你可能就需要选择先强行切换，事后再补数据的策略。

当然，事后复盘的时候，我们想到了一个改进措施就是，让业务逻辑不要依赖于这类日志的写入。也就是说，日志写入这个逻辑模块应该可以降级，比如写到本地文件，或者写到另外一个临时库里面。

这样的话，这种场景就又可以使用可靠性优先策略了。

接下来我们再看看，按照可靠性优先的思路，异常切换会是什么效果？

假设，主库 A 和备库 B 间的主备延迟是 30 分钟，这时候主库 A 掉电了，HA 系统要切换 B 作为主库。我们在主动切换的时候，可以等到主备延迟小于 5 秒的时候再启动切换，但这时候已经别无选择了。
```

![image-20220125140516718](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220125140516718.png)

​																							**图 5 可靠性优先策略，主库不可用**

```
采用可靠性优先策略的话，你就必须得等到备库 B 的 seconds_behind_master=0 之后，才能切换。但现在的情况比刚刚更严重，并不是系统只读、不可写的问题了，而是系统处于完全不可用的状态。因为，主库 A 掉电后，我们的连接还没有切到备库 B。

你可能会问，那能不能直接切换到备库 B，但是保持 B 只读呢？

这样也不行。

因为，这段时间内，中转日志还没有应用完成，如果直接发起主备切换，客户端查询看不到之前执行完成的事务，会认为有“数据丢失”。

虽然随着中转日志的继续应用，这些数据会恢复回来，但是对于一些业务来说，查询到“暂时丢失数据的状态”也是不能被接受的。

聊到这里你就知道了，在满足数据可靠性的前提下，MySQL 高可用系统的可用性，是依赖于主备延迟的。延迟的时间越小，在主库故障的时候，服务恢复需要的时间就越短，可用性就越高。
```



## 备库为什么会延迟好几个小时？

```
几种可能导致备库延迟的场景。会发现，这些场景里，不论是偶发性的查询压力，还是备份，对备库延迟的影响一般是分钟级的，而且在备库恢复正常以后都能够追上来。

但是，如果备库执行日志的速度持续低于主库生成日志的速度，那这个延迟就有可能成小时级别。而且对于一个压力持续比较高的主库来说，备库很可能永远都追不上主库的节奏。

这就涉及到：备库并行复制能力。
```

![image-20220125141144694](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220125141144694.png)

​																													**图 1 主备流程图**

```
谈到主备的并行复制能力，我们要关注的是图中黑色的两个箭头。一个箭头代表了客户端写入主库，另一箭头代表的是备库上 sql_thread 执行中转日志（relay log）。如果用箭头的粗细来代表并行度的话，那么真实情况就如图 1 所示，第一个箭头要明显粗于第二个箭头。

在主库上，影响并发度的原因就是各种锁了。由于 InnoDB 引擎支持行锁，除了所有并发事务都在更新同一行（热点行）这种极端场景外，它对业务并发度的支持还是很友好的。所以，你在性能测试的时候会发现，并发压测线程 32 就比单线程时，总体吞吐量高。

而日志在备库上的执行，就是图中备库上 sql_thread 更新数据 (DATA) 的逻辑。如果是用单线程的话，就会导致备库应用日志不够快，造成主备延迟。

在官方的 5.6 版本之前，MySQL 只支持单线程复制，由此在主库并发高、TPS 高时就会出现严重的主备延迟问题。

从单线程复制到最新版本的多线程复制，中间的演化经历了好几个版本。接下来，我就跟你说说 MySQL 多线程复制的演进过程。
```

```
其实说到底，所有的多线程复制机制，都是要把图 1 中只有一个线程的 sql_thread，拆成多个线程，也就是都符合下面的这个模型：
```

![image-20220125141302912](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220125141302912.png)

​																												**图 2 多线程模型**

```
图 2 中，coordinator 就是原来的 sql_thread, 不过现在它不再直接更新数据了，只负责读取中转日志和分发事务。真正更新日志的，变成了 worker 线程。而 work 线程的个数，就是由参数 slave_parallel_workers 决定的。根据我的经验，把这个值设置为 8~16 之间最好（32 核物理机的情况），毕竟备库还有可能要提供读查询，不能把 CPU 都吃光了。

接下来，你需要先思考一个问题：事务能不能按照轮询的方式分发给各个 worker，也就是第一个事务分给 worker_1，第二个事务发给 worker_2 呢？

其实是不行的。因为，事务被分发给 worker 以后，不同的 worker 就独立执行了。但是，由于 CPU 的调度策略，很可能第二个事务最终比第一个事务先执行。而如果这时候刚好这两个事务更新的是同一行，也就意味着，同一行上的两个事务，在主库和备库上的执行顺序相反，会导致主备不一致的问题。

接下来，请你再设想一下另外一个问题：同一个事务的多个更新语句，能不能分给不同的 worker 来执行呢？

答案是，也不行。举个例子，一个事务更新了表 t1 和表 t2 中的各一行，如果这两条更新语句被分到不同 worker 的话，虽然最终的结果是主备一致的，但如果表 t1 执行完成的瞬间，备库上有一个查询，就会看到这个事务“更新了一半的结果”，破坏了事务逻辑的隔离性。

所以，coordinator 在分发的时候，需要满足以下这两个基本要求：
1.不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个 worker 中。
2.同一个事务不能被拆开，必须放到同一个 worker 中。

各个版本的多线程复制，都遵循了这两条基本原则。接下来，我们就看看各个版本的并行复制策略。
```



### MYSQL5.5版本的并行复制策略

```
官方 MySQL 5.5 版本是不支持并行复制的。但是，在 2012 年的时候，我自己服务的业务出现了严重的主备延迟，原因就是备库只有单线程复制。然后，我就先后写了两个版本的并行策略。

这里，我给你介绍一下这两个版本的并行策略，即按表分发策略和按行分发策略，以帮助你理解 MySQL 官方版本并行复制策略的迭代。
```



#### 按表分发策略

```
按表分发事务的基本思路是，如果两个事务更新不同的表，它们就可以并行。因为数据是存储在表里的，所以按表分发，可以保证两个 worker 不会更新同一行。

当然，如果有跨表的事务，还是要把两张表放在一起考虑的。如图 3 所示，就是按表分发的规则。
```

![image-20220125141549886](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220125141549886.png)

​																									**图 3 按表并行复制程模型**

```
可以看到，每个 worker 线程对应一个 hash 表，用于保存当前正在这个 worker 的“执行队列”里的事务所涉及的表。hash 表的 key 是“库名. 表名”，value 是一个数字，表示队列中有多少个事务修改这个表。

在有事务分配给 worker 时，事务里面涉及的表会被加到对应的 hash 表中。worker 执行完成后，这个表会被从 hash 表中去掉。

图 3 中，hash_table_1 表示，现在 worker_1 的“待执行事务队列”里，有 4 个事务涉及到 db1.t1 表，有 1 个事务涉及到 db2.t2 表；hash_table_2 表示，现在 worker_2 中有一个事务会更新到表 t3 的数据。

假设在图中的情况下，coordinator 从中转日志中读入一个新事务 T，这个事务修改的行涉及到表 t1 和 t3。

现在我们用事务 T 的分配流程，来看一下分配规则。
1.由于事务 T 中涉及修改表 t1，而 worker_1 队列中有事务在修改表 t1，事务 T 和队列中的某个事务要修改同一个表的数据，这种情况我们说事务 T 和 worker_1 是冲突的。
2.按照这个逻辑，顺序判断事务 T 和每个 worker 队列的冲突关系，会发现事务 T 跟 worker_2 也冲突。
3.事务 T 跟多于一个 worker 冲突，coordinator 线程就进入等待。
4.每个 worker 继续执行，同时修改 hash_table。假设 hash_table_2 里面涉及到修改表 t3 的事务先执行完成，就会从 hash_table_2 中把 db1.t3 这一项去掉。
5.这样 coordinator 会发现跟事务 T 冲突的 worker 只有 worker_1 了，因此就把它分配给 worker_1。
6.coordinator 继续读下一个中转日志，继续分配事务。

也就是说，每个事务在分发的时候，跟所有 worker 的冲突关系包括以下三种情况：
1.如果跟所有 worker 都不冲突，coordinator 线程就会把这个事务分配给最空闲的 woker;
2.如果跟多于一个 worker 冲突，coordinator 线程就进入等待状态，直到和这个事务存在冲突关系的 worker 只剩下 1 个；
3.如果只跟一个 worker 冲突，coordinator 线程就会把这个事务分配给这个存在冲突关系的 worker。

这个按表分发的方案，在多个表负载均匀的场景里应用效果很好。但是，如果碰到热点表，比如所有的更新事务都会涉及到某一个表的时候，所有事务都会被分配到同一个 worker 中，就变成单线程复制了。
```



#### 按行分发策略

```
要解决热点表的并行复制问题，就需要一个按行并行复制的方案。按行复制的核心思路是：如果两个事务没有更新相同的行，它们在备库上可以并行执行。显然，这个模式要求 binlog 格式必须是 row。

这时候，我们判断一个事务 T 和 worker 是否冲突，用的就规则就不是“修改同一个表”，而是“修改同一行”。

按行复制和按表复制的数据结构差不多，也是为每个 worker，分配一个 hash 表。只是要实现按行分发，这时候的 key，就必须是“库名 + 表名 + 唯一键的值”。

但是，这个“唯一键”只有主键 id 还是不够的，我们还需要考虑下面这种场景，表 t1 中除了主键，还有唯一索引 a：
```

```sql
CREATE TABLE `t1` (
  `id` int(11) NOT NULL,
  `a` int(11) DEFAULT NULL,
  `b` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `a` (`a`)
) ENGINE=InnoDB;
 
insert into t1 values(1,1,1),(2,2,2),(3,3,3),(4,4,4),(5,5,5);
```

![image-20220125142459155](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220125142459155.png)

​																										**图 4 唯一键冲突示例**

```
可以看到，这两个事务要更新的行的主键值不同，但是如果它们被分到不同的 worker，就有可能 session B 的语句先执行。这时候 id=1 的行的 a 的值还是 1，就会报唯一键冲突。

因此，基于行的策略，事务 hash 表中还需要考虑唯一键，即 key 应该是“库名 + 表名 + 索引 a 的名字 +a 的值”。

比如，在上面这个例子中，我要在表 t1 上执行 update t1 set a=1 where id=2 语句，在 binlog 里面记录了整行的数据修改前各个字段的值，和修改后各个字段的值。

因此，coordinator 在解析这个语句的 binlog 的时候，这个事务的 hash 表就有三个项:
1.key=hash_func(db1+t1+“PRIMARY”+2), value=2; 这里 value=2 是因为修改前后的行 id 值不变，出现了两次。
2.key=hash_func(db1+t1+“a”+2), value=1，表示会影响到这个表 a=2 的行。
3.key=hash_func(db1+t1+“a”+1), value=1，表示会影响到这个表 a=1 的行。

可见，相比于按表并行分发策略，按行并行策略在决定线程分发的时候，需要消耗更多的计算资源。你可能也发现了，这两个方案其实都有一些约束条件：
1.要能够从 binlog 里面解析出表名、主键值和唯一索引的值。也就是说，主库的 binlog 格式必须是 row；
2.表必须有主键；
3.不能有外键。表上如果有外键，级联更新的行不会记录在 binlog 中，这样冲突检测就不准确。

但，好在这三条约束规则，本来就是 DBA 之前要求业务开发人员必须遵守的线上使用规范，所以这两个并行复制策略在应用上也没有碰到什么麻烦。

对比按表分发和按行分发这两个方案的话，按行分发策略的并行度更高。不过，如果是要操作很多行的大事务的话，按行分发的策略有两个问题：
1.耗费内存。比如一个语句要删除 100 万行数据，这时候 hash 表就要记录 100 万个项。
2.耗费 CPU。解析 binlog，然后计算 hash 值，对于大事务，这个成本还是很高的。

所以，我在实现这个策略的时候会设置一个阈值，单个事务如果超过设置的行数阈值（比如，如果单个事务更新的行数超过 10 万行），就暂时退化为单线程模式，退化过程的逻辑大概是这样的：
1.coordinator 暂时先 hold 住这个事务；
2.等待所有 worker 都执行完成，变成空队列；
3.coordinator 直接执行这个事务；
4.恢复并行模式。

读到这里，你可能会感到奇怪，这两个策略又没有被合到官方，我为什么要介绍这么详细呢？其实，介绍这两个策略的目的是抛砖引玉，方便你理解后面要介绍的社区版本策略。
```



### MySQL 5.6 版本的并行复制策略

```
官方 MySQL5.6 版本，支持了并行复制，只是支持的粒度是按库并行。理解了上面介绍的按表分发策略和按行分发策略，你就理解了，用于决定分发策略的 hash 表里，key 就是数据库名。

这个策略的并行效果，取决于压力模型。如果在主库上有多个 DB，并且各个 DB 的压力均衡，使用这个策略的效果会很好。

相比于按表和按行分发，这个策略有两个优势：
1.构造 hash 值的时候很快，只需要库名；而且一个实例上 DB 数也不会很多，不会出现需要构造 100 万个项这种情况。
2.不要求 binlog 的格式。因为 statement 格式的 binlog 也可以很容易拿到库名。

但是，如果你的主库上的表都放在同一个 DB 里面，这个策略就没有效果了；或者如果不同 DB 的热点不同，比如一个是业务逻辑库，一个是系统配置库，那也起不到并行的效果。

理论上你可以创建不同的 DB，把相同热度的表均匀分到这些不同的 DB 中，强行使用这个策略。不过据我所知，由于需要特地移动数据，这个策略用得并不多。
```



### MariaDB 的并行复制策略

```

```



## 主从模式

![image-20220211100834025](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220211100834025.png)

​																										**图 1 一主多从基本结构**

```
图中，虚线箭头表示的是主备关系，也就是 A 和 A’互为主备， 从库 B、C、D 指向的是主库 A。一主多从的设置，一般用于读写分离，主库负责所有的写入和一部分读，其他的读请求则由从库分担。
如下：在一主多从架构下，主库故障后的主备切换问题。
```

![image-20220211101002092](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220211101002092.png)

​																									**图 2 一主多从基本结构 -- 主备切换**

```
相比于一主一备的切换流程，一主多从结构在切换完成后，A’会成为新的主库，从库 B、C、D 也要改接到 A’。正是由于多了从库 B、C、D 重新指向的这个过程，所以主备切换的复杂性也相应增加了。
```



### 基于位点的主备切换







## 答疑

### 日志相关问题

#### 问题一

**在两阶段提交的不同瞬间，MySQL 如果发生异常重启，是怎么保证数据完整性的？**

![两阶段提交](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/两阶段提交.jpg)

​																												**图 1 两阶段提交示意图**

```
在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象？
1.如果在图中时刻 A 的地方，也就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash），由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。到这里，大家都可以理解。

2.也就是 binlog 写完，redo log 还没 commit 前发生 crash，那崩溃恢复的时候 MySQL 会怎么处理？
我们先来看一下崩溃恢复时的判断规则。
	2.1.如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交；
	2.2.如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：
		2.2.1.a. 如果是，则提交事务；
		2.2.2.b. 否则，回滚事务。
这里，时刻 B 发生 crash 对应的就是 2(a) 的情况，崩溃恢复过程中事务会被提交。
```

```
追问 1：MySQL 怎么知道 binlog 是完整的?
答：一个事务的 binlog 是有完整格式的：
1.statement 格式的 binlog，最后会有 COMMIT；
2.row 格式的 binlog，最后会有一个 XID event。
在 MySQL 5.6.2 版本以后，还引入了 binlog-checksum 参数，用来验证 binlog 内容的正确性。对于 binlog 日志由于磁盘原因，可能会在日志中间出错的情况，MySQL 可以通过校验 checksum 的结果来发现。所以，MySQL 还是有办法验证事务 binlog 的完整性的。
```

```
追问 2：redo log 和 binlog 是怎么关联起来的?
答：它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log：
1.如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；
2.如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。
```

```
追问 3：处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计?
答：其实，这个问题还是跟数据与备份的一致性有关。在时刻 B，也就是 binlog 写完以后 MySQL 发生崩溃，这时候 binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。
所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。
```

```
追问 4：如果这样的话，为什么还要两阶段提交呢？干脆先 redo log 写完，再写 binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？
答：其实，两阶段提交是经典的分布式系统问题，并不是 MySQL 独有的。

如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。

对于 InnoDB 引擎来说，如果 redo log 提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果 redo log 直接提交，然后 binlog 写入的时候失败，InnoDB 又回滚不了，数据和 binlog 日志又不一致了。

两阶段提交就是为了给所有人一个机会，当每个人都说“我 ok”的时候，再一起提交。
```

```
追问 5：不引入两个日志，也就没有两阶段提交的必要了。只用 binlog 来支持崩溃恢复，又能支持归档，不就可以了？
答：流程：“数据更新到内存” -> “写 binlog” -> “提交事务”，是不是也可以提供崩溃恢复的能力？
是不具备崩溃恢复的能力的。
历史原因：
InnoDB 并不是 MySQL 的原生存储引擎。MySQL 的原生引擎是 MyISAM，设计之初就有没有支持崩溃恢复。

InnoDB 在作为 MySQL 的插件加入 MySQL 引擎家族之前，就已经是一个提供了崩溃恢复和事务支持的引擎了。

InnoDB 接入了 MySQL 后，发现既然 binlog 没有崩溃恢复的能力，那就用 InnoDB 原有的 redo log 好了。

如果没有 redo log 如下图：
```

![image-20220112164531323](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220112164531323.png)

​																									**图 2 只用 binlog 支持崩溃恢复**

```
这样的流程下，binlog 还是不能支持崩溃恢复的。我说一个不支持的点吧：binlog 没有能力恢复“数据页”。
如果在图中标的位置，也就是 binlog2 写完了，但是整个事务还没有 commit 的时候，MySQL 发生了 crash。
重启后，引擎内部事务 2 会回滚，然后应用 binlog2 可以补回来；但是对于事务 1 来说，系统已经认为提交完成了，不会再应用一次 binlog1。
但是，InnoDB 引擎使用的是 WAL 技术，执行事务的时候，写完内存和日志，事务就算完成了。如果之后崩溃，要依赖于日志来恢复数据页。
也就是说在图中这个位置发生崩溃的话，事务 1 也是可能丢失了的，而且是数据页级的丢失。此时，binlog 里面并没有记录数据页的更新细节，是补不回来的。
你如果要说，那我优化一下 binlog 的内容，让它来记录数据页的更改可以吗？但，这其实就是又做了一个 redo log 出来。
所以，至少现在的 binlog 能力，还不能支持崩溃恢复。
```

```
追问 6：能不能反过来，只用 redo log，不要 binlog？
答：如果只从崩溃恢复的角度来讲是可以的。你可以把 binlog 关掉，这样就没有两阶段提交了，但系统依然是 crash-safe 的。
但是，如果你了解一下业界各个公司的使用场景的话，就会发现在正式的生产库上，binlog 都是开着的。因为 binlog 有着 redo log 无法替代的功能。
一个是归档。redo log 是循环写，写到末尾是要回到开头继续写的。这样历史日志没法保留，redo log 也就起不到归档的作用。
一个就是 MySQL 系统依赖于 binlog。binlog 作为 MySQL 一开始就有的功能，被用在了很多地方。其中，MySQL 系统高可用的基础，就是 binlog 复制。
还有很多公司有异构系统（比如一些数据分析系统），这些系统就靠消费 MySQL 的 binlog 来更新自己的数据。关掉 binlog 的话，这些下游系统就没法输入了。
总之，由于现在包括 MySQL 高可用在内的很多系统机制都依赖于 binlog。
```

```
追问 7：redo log 一般设置多大？
答：redo log 太小的话，会导致很快就被写满，然后不得不强行刷 redo log，这样 WAL 机制的能力就发挥不出来了。

所以，如果是现在常见的几个 TB 的磁盘的话，就不要太小气了，直接将 redo log 设置为 4 个文件、每个文件 1GB 吧。
```

```
追问 8：正常运行中的实例，数据写入后的最终落盘，是从 redo log 更新过来的还是从 buffer pool 更新过来的呢？
回答：这个问题其实问得非常好。这里涉及到了，“redo log 里面到底是什么”的问题。
实际上，redo log 并没有记录数据页的完整数据，所以它并没有能力自己去更新磁盘数据页，也就不存在“数据最终落盘，是由 redo log 更新过去”的情况。
1.如果是正常运行的实例的话，数据页被修改以后，跟磁盘的数据页不一致，称为脏页。最终数据落盘，就是把内存中的数据页写盘。这个过程，甚至与 redo log 毫无关系。
2.在崩溃恢复场景中，InnoDB 如果判断到一个数据页可能在崩溃恢复的时候丢失了更新，就会将它读到内存，然后让 redo log 更新内存内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。
```

```
追问 9：redo log buffer 是什么？是先修改内存，还是先写 redo log 文件？
在一个事务的更新过程中，日志是要写多次的。比如下面这个事务：
begin;
insert into t1 ...
insert into t2 ...
commit;

这个事务要往两个表中插入记录，插入数据的过程中，生成的日志都得先保存起来，但又不能在还没 commit 的时候就直接写到 redo log 文件里。
所以，redo log buffer 就是一块内存，用来先存 redo 日志的。也就是说，在执行第一个 insert 的时候，数据的内存被修改了，redo log buffer 也写入了日志。
但是，真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。
（这里说的是事务执行过程中不会“主动去刷盘”，以减少不必要的 IO 消耗。但是可能会出现“被动写入磁盘”，比如内存不够、其他事务提交等情况。
单独执行一个更新语句的时候，InnoDB 会自己启动一个事务，在语句执行完成的时候提交。过程跟上面是一样的，只不过是“压缩”到了一个语句里面完成。
```



### 业务设计问题

#### 问题一

```
业务上有这样的需求，A、B 两个用户，如果互相关注，则成为好友。设计上是有两张表，一个是 like 表，一个是 friend 表，like 表有 user_id、liker_id 两个字段，我设置为复合唯一索引即 uk_user_id_liker_id。语句执行逻辑是这样的：

以 A 关注 B 为例：
第一步，先查询对方有没有关注自己（B 有没有关注 A）
select * from like where user_id = B and liker_id = A;

如果有，则成为好友
insert into friend;

没有，则只是单向关注关系
insert into like;

但是如果 A、B 同时关注对方，会出现不会成为好友的情况。因为上面第 1 步，双方都没关注对方。第 1 步即使使用了排他锁也不行，因为记录不存在，行锁无法生效。请问这种情况，在 MySQL 锁层面有没有办法处理？
```

```sql
CREATE TABLE `like` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` int(11) NOT NULL,
  `liker_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `uk_user_id_liker_id` (`user_id`,`liker_id`)
) ENGINE=InnoDB;
 
CREATE TABLE `friend` (
  id` int(11) NOT NULL AUTO_INCREMENT,
  `friend_1_id` int(11) NOT NULL,
  `firned_2_id` int(11) NOT NULL,
  UNIQUE KEY `uk_friend` (`friend_1_id`,`firned_2_id`)
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
```

```
疑问：在并发场景下，同时有两个人，设置为关注对方，就可能导致无法成功加为朋友关系。
```

![image-20230306105508775](https://renyh-bucket-1302033757.cos.ap-chengdu.myqcloud.com/images/image-20230306105508775.png)

​																								**图 3 并发“喜欢”逻辑操作顺序**

```
由于一开始 A 和 B 之间没有关注关系，所以两个事务里面的 select 语句查出来的结果都是空。
因此，session 1 的逻辑就是“既然 B 没有关注 A，那就只插入一个单向关注关系”。session 2 也同样是这个逻辑。
这个结果对业务来说就是 bug 了。因为在业务设定里面，这两个逻辑都执行完成以后，是应该在 friend 表里面插入一行记录的。
【注】：第 1 步即使使用了排他锁也不行，因为记录不存在，行锁无法生效
```

```
解决方案：
要给“like”表增加一个字段，比如叫作 relation_ship，并设为整型，取值 1、2、3。
1.值是 1 的时候，表示 user_id 关注 liker_id;
2.值是 2 的时候，表示 liker_id 关注 user_id;
3.值是 3 的时候，表示互相关注。
```

```sql
-- A 关注 B（应用代码里面，比较 A 和 B 的大小，如果 A<B，就执行下面的逻辑）
begin; /* 启动事务 */
insert into `like`(user_id, liker_id, relation_ship) values(A, B, 1) on duplicate key update relation_ship=relation_ship | 1;
select relation_ship from `like` where user_id=A and liker_id=B;
/* 代码中判断返回的 relation_ship，
  如果是 1，事务结束，执行 commit
  如果是 3，则执行下面这两个语句：
  */
insert ignore into friend(friend_1_id, friend_2_id) values(A,B);
commit;

-- 如果 A>B，则执行下面的逻辑
begin; /* 启动事务 */
insert into `like`(user_id, liker_id, relation_ship) values(B, A, 2) on duplicate key update relation_ship=relation_ship | 2;
select relation_ship from `like` where user_id=B and liker_id=A;
/* 代码中判断返回的 relation_ship，
  如果是 2，事务结束，执行 commit
  如果是 3，则执行下面这两个语句：
*/
insert ignore into friend(friend_1_id, friend_2_id) values(B,A);
commit;
```

```
这个设计里，让“like”表里的数据保证 user_id < liker_id，这样不论是 A 关注 B，还是 B 关注 A，在操作“like”表的时候，如果反向的关系已经存在，就会出现行锁冲突。然后，insert … on duplicate 语句，确保了在事务内部，执行了这个 SQL 语句后，就强行占住了这个行锁，之后的 select 判断 relation_ship 这个逻辑时就确保了是在行锁保护下的读操作。

操作符 “|” 是按位或，连同最后一句 insert 语句里的 ignore，是为了保证重复调用时的幂等性。

这样，即使在双方“同时”执行关注操作，最终数据库里的结果，也是 like 表里面有一条关于 A 和 B 的记录，而且 relation_ship 的值是 3， 并且 friend 表里面也有了 A 和 B 的这条记录。
```



### 锁相关问题

```
为了方便你理解，我们再一起复习一下加锁规则。这个规则中，包含了两个“原则”、两个“优化”和一个“bug”：
1.原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。
2.原则 2：查找过程中访问到的对象才会加锁。
3.优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。
4.优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。
5.一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。
```

```sql
CREATE TABLE `g` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  `d` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `c` (`c`)
) ENGINE=InnoDB;
 
insert into g values(0,0,0),(5,5,5),
(10,10,10),(15,15,15),(20,20,20),(25,25,25);
```



#### 不等号条件里的等值查询

```sql
begin;
select * from t where id>9 and id<12 order by id desc for update;
```

```
利用上面的加锁规则，我们知道这个语句的加锁范围是主键索引上的 (0,5]、(5,10] 和 (10, 15)。也就是说，id=15 这一行，并没有被加上行锁。为什么呢？

我们说加锁单位是 next-key lock，都是前开后闭区间，但是这里用到了优化 2，即索引上的等值查询，向右遍历的时候 id=15 不满足条件，所以 next-key lock 退化为了间隙锁 (10, 15)。

但是，我们的查询语句中 where 条件是大于号和小于号，这里的“等值查询”又是从哪里来的呢？

要知道，加锁动作是发生在语句执行过程中的，所以你在分析加锁行为的时候，要从索引上的数据结构开始。这里，我再把这个过程拆解一下。

如图 1 所示，是这个表的索引 id 的示意图。
```

![image-20220125144033732](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220125144033732.png)

​																											**图 1 索引 id 示意图**

```
1.首先这个查询语句的语义是 order by id desc，要拿到满足条件的所有行，优化器必须先找到“第一个 id<12 的值”。
2.这个过程是通过索引树的搜索过程得到的，在引擎内部，其实是要找到 id=12 的这个值，只是最终没找到，但找到了 (10,15) 这个间隙。
3.然后向左遍历，在遍历过程中，就不是等值查询了，会扫描到 id=5 这一行，所以会加一个 next-key lock (0,5]。

也就是说，在执行过程中，通过树搜索的方式定位记录的时候，用的是“等值查询”的方法。
```



#### 等值查询的过程

```sql
begin;
select id from t where c in(5,20,10) lock in share mode;
```

![image-20220125144445565](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/image-20220125144445565.png)

​																											**图 2 in 语句的 explain 结果**

```
可以看到，这条 in 语句使用了索引 c 并且 rows=3，说明这三个值都是通过 B+ 树搜索定位的。

在查找 c=5 的时候，先锁住了 (0,5]。但是因为 c 不是唯一索引，为了确认还有没有别的记录 c=5，就要向右遍历，找到 c=10 才确认没有了，这个过程满足优化 2，所以加了间隙锁 (5,10)。

同样的，执行 c=10 这个逻辑的时候，加锁的范围是 (5,10] 和 (10,15)；执行 c=20 这个逻辑的时候，加锁的范围是 (15,20] 和 (20,25)。

通过这个分析，我们可以知道，这条语句在索引 c 上加的三个记录锁的顺序是：先加 c=5 的记录锁，再加 c=10 的记录锁，最后加 c=20 的记录锁。

你可能会说，这个加锁范围，不就是从 (5,25) 中去掉 c=15 的行锁吗？为什么这么麻烦地分段说呢？

因为我要跟你强调这个过程：这些锁是“在执行过程中一个一个加的”，而不是一次性加上去的。

理解了这个加锁过程之后，我们就可以来分析下面例子中的死锁问题了。
```

```sql
-- 如果同时有另外一个语句，是这么写的：

select id from t where c in(5,20,10) order by c desc for update;
```

```
此时的加锁范围，又是什么呢？

我们现在都知道间隙锁是不互锁的，但是这两条语句都会在索引 c 上的 c=5、10、20 这三行记录上加记录锁。

这里你需要注意一下，由于语句里面是 order by c desc， 这三个记录锁的加锁顺序，是先锁 c=20，然后 c=10，最后是 c=5。

也就是说，这两条语句要加锁相同的资源，但是加锁顺序相反。当这两条语句并发执行的时候，就可能出现死锁。
```



#### 怎么看死锁？

```
图 3 是在出现死锁后，执行 show engine innodb status 命令得到的部分输出。这个命令会输出很多信息，有一节 LATESTDETECTED DEADLOCK，就是记录的最后一次死锁信息。
```

![死锁现场](http://ren-bed.oss-cn-beijing.aliyuncs.com/img/死锁现场.png)

​																														**图 3 死锁现场**

```

```



```
Mysql为什么最终选择B+树而不是B树？
对于树结构来说，影响查找的效率就是树的高度，B+树相对于B树来说，存储相同的数据量树的高度要低（B树每个节点都存有数据，而B+树只有叶子节点存储真正的数据，叶子节点之间有指针连接，可以提高性能，决定B+树高度的就是节点存储索引的数量）
```

